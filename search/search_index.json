{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"LTRDCN-1572 Welcome to Cisco Live LTRDCN-1572: VXLAN EVPN Fabric and automation using Ansible. For full documentation visit Cisco Live . Speakers Faisal Chaudhry Distinguished Engineer, Cisco Customer Experience (CX) Lei Tian Solutions Architect, Cisco Customer Experience","title":"Home"},{"location":"#ltrdcn-1572","text":"Welcome to Cisco Live LTRDCN-1572: VXLAN EVPN Fabric and automation using Ansible. For full documentation visit Cisco Live .","title":"LTRDCN-1572"},{"location":"#speakers","text":"Faisal Chaudhry Distinguished Engineer, Cisco Customer Experience (CX) Lei Tian Solutions Architect, Cisco Customer Experience","title":"Speakers"},{"location":"Task1-ansible-node/","text":"Your first task will be to build an Ansible node on a server running redhat CentOS operating system. At the end of this task, you will have a fully operational Ansible node. Step 1: Connect to lab using anyconnect VPN You will connect to dcloud-rtp-anyconnect.cisco.com using Cisco VPN AnyConnect client, as shown in below picture, with the username and password provided by the lab admin. Note: lab admin will furnish the credentials information to the participant. If you don't have this information please ask the lab speakers. Step 2: Enter VPN credentials After prompted for credentials, use the credentials provided by the lab admin. \u2022 Below is an example of user logging into a reference POD: Hit accept when the prompt appears to accept the VPN connection login Step 3: RDP to workstation In this step, you will connect to the workstation with RDP client on your machines. Use below details for this RDP session: Workstation: 198.18.133.36 Username: dcloud\\demouser Password: C1sco12345 Below screenshot is only an example for this RDP connection: Step 4: MTputty Once you have the RDP session to the remote workstation, then you will use MTputty client to connect to all devices in this lab. MTputty is already installed on the Desktop of the workstation where you connected using RDP. Run this application by clicking on the icon on the desktop: Step 5: SSH into Ansible node SSH into Ansible node (198.18.134.150) by double clicking the Ansible icon on the left pan with username root and password C1sco12345 Step 6: Verify Python Once successfully SSH into the ansible node, the very first thing we are going to do after logging into Ansible server is verify the python version by running python --version command - as shown below: [root@rhel7-tools ~]# python --version It is an important step as we need minimum 2.7.5 version of python in order to install some features for ansbile. The output of above command confirms this version. Ansible can be run from any machine with Python 2 (versions 2.6 or 2.7) or Python 3 (versions 3.5 and higher) installed. Step 7: Install PIP After verifying we have the minimum version of python installed, we are now going to Install PIP python package using easy_install pip command as shown below: [root@rhel7-tools ~]# easy_install pip Below screenshot shows the execution of above command: Next, update pip to latest version by executing command pip install --upgrade pip as shown below: [root@rhel7-tools ~]# pip install --upgrade pip Note: you can ignore the warning related to python version. Below screenshot shows the exection of above command: After installing PIP package, we are going to add the relevant packages that are needed for this Ansible based VXLAN lab. Below are the packages required for this lab. Paramiko PyYAML Jinj2 Httplib2 Run the command pip install paramiko PyYAML jinja2 httplib2 , as shown below, to install these packages: [root@rhel7-tools ~]# pip install paramiko PyYAML jinja2 httplib2 Below screenshot shows the execution of above command: As a final step, we are going to install Ansible on this RHEL. Once the install is initiated by using pip install ansible==2.8.0b1 command (as shown below). It may take few minutes for it to download and install. [root@rhel7-tools ~]# pip install ansible==2.8.0b1 Below screenshot shows the execution of above command: Step 8: Verify Ansible After installation is complete, check Ansible version by executing command ansible --version , as shown below: [root@rhel7-tools ~]# ansible --version Below screenshot shows the execution of above command: Step 9: Create Ansible Inventory Now, we are going to create inventory, host variables and Configuration file. This is important as Ansible works against multiple systems in the system by selecting portions of systems listed in Ansible inventory. Similarly, configuration settings in Ansible are adjustable via configuration file. Create folder named LTRDCN-1572 as working environment and verify that it\u2019s empty: [root@rhel7-tools ~]# mkdir LTRDCN-1572 && cd LTRDCN-1572 [root@rhel7-tools LTRDCN-1572]# ls Below screenshot shows the execution of above command: Next: Create Ansible inventory file to include Spine and Leaf switches. By default Ansible has inventory file saved in location /etc/ansible/hosts. In this lab we will create hosts file in the working environment. This file is created from ansible host prompt using cat command. Note: you should copy and paste the complete section below i.e., starting from cat till EOF (also shown in subsequent screenshot): cat << EOF >> hosts #define global variables, groups and host variables [all:vars] ansible_connection = local user=admin pwd=C1sco12345 gather_fact=no [jinja2_spine] 198.18.4.202 [jinja2_leaf] 198.18.4.104 [spine] 198.18.4.201 [leaf] 198.18.4.101 198.18.4.103 [server] 198.18.134.50 eth1=172.21.140.10 gw=172.21.140.1 198.18.134.52 eth1=172.21.140.11 gw=172.21.140.1 198.18.134.53 eth1=172.21.141.11 gw=172.21.141.1 EOF Below screenshot shows the output of above command: Now you may verify the content of this file using below command: [root@rhel7-tools LTRDCN-1572]# more hosts Below screenshot shows the execution of above command: Create Ansible config (ansible.cfg) using the same steps as above. Note: you should copy and paste the complete section below i.e., starting from cat till EOF (as shown in below screenshot): cat << EOF >> ansible.cfg [defaults] inventory = hosts host_key_checking = false record_host_key = true stdout_callback = debug deprecation_warnings = False EOF Now you may verify the content of this file using below command: [root@rhel7-tools LTRDCN-1572]# more ansible.cfg Below screenshot shows the output of above commands: Do ls to verify the file that you just created under project folder LTRDCN-1572. Below screenshot shows the execution of above command: Create host variable folder named host_vars in folder LTRDCN-1572 by using below command. In this lab, we will use host_vars file to define the variables for various hosts (in next bullets): [root@rhel7-tools LTRDCN-1572]# mkdir host_vars && cd host_vars Create host variable file for each host in inventory by using the cat command. The variable file for a switch is created using the below cat command. Note: you can copy and paste starting from cat till EOF (as shown in below screenshot). Note: the spaces in the file are important so do not remove those: cat << EOF >> 198.18.4.101.yml --- hostname: leaf_1 loopback0: 192.168.0.8 loopback1: 192.168.0.18 router_id: 192.168.0.8 EOF Below screenshot shows the execution of above command: Now you may verify the content of this file using more 198.18.4.101.yml as shown below: [root@rhel7-tools LTRDCN-1572]# more 198.18.4.101.yml Below screenshot shows the execution of above command: Create a new host variable file for next host in inventory. The variable file for a switch is created using the below cat command. Note: you can copy and paste starting from cat till EOF (as shown in below screenshot). Note: the spaces in the file are important so do not remove those: cat << EOF >> 198.18.4.103.yml --- hostname: leaf_3 loopback0: 192.168.0.10 loopback1: 192.168.0.110 router_id: 192.168.0.10 EOF Below screenshot shows the execution of above command: Now you may verify the content of this file using more 198.18.4.103.yml as shown below: [root@rhel7-tools LTRDCN-1572]# more 198.18.4.103.yml Below screenshot shows the execution of above command: Create a new host variable file for next host in inventory. The variable file for a switch is created using the below cat command. Note: you can copy and paste starting from cat till EOF (as shown in below screenshot). Note: the spaces in the file are important so do not remove those: cat << EOF >> 198.18.4.104.yml --- hostname: leaf_4 loopback0: 192.168.0.11 loopback1: 192.168.0.111 router_id: 192.168.0.11 EOF Now you may verify the content of this file using more 198.18.4.104.yml as shown below: [root@rhel7-tools LTRDCN-1572]# more 198.18.4.104.yml Below screenshot shows the execution of above commands: Create a new host variable file for next host in inventory. The variable file for a switch is created using the below cat command. Note: you can copy and paste starting from cat till EOF (as shown in below screenshot). Note: the spaces in the file are important so do not remove those: cat << EOF >> 198.18.4.201.yml --- hostname: spine-1 loopback0: 192.168.0.6 loopback1: 192.168.0.100 router_id: 192.168.0.6 EOF Now you may verify the content of this file using more 198.18.4.201.yml as shown below: [root@rhel7-tools LTRDCN-1572]# more 198.18.4.201.yml Below screenshot shows the execution of above commands: Create a new host variable file for next host in inventory. The variable file for a switch is created using the below cat command. Note: you can copy and paste starting from cat till EOF (as shown in below screenshot). Note: the spaces in the file are important so do not remove those: cat << EOF >> 198.18.4.202.yml --- hostname: spine-2 loopback0: 192.168.0.7 loopback1: 192.168.0.100 router_id: 192.168.0.7 EOF Now you may verify the content of this file using more 198.18.4.202.yml as shown below: [root@rhel7-tools LTRDCN-1572]# more 198.18.4.202.yml Below screenshot shows the execution of above commands: Step 10: Ansible role structure Role is very useful technique to manage a set of playbooks in Ansible. In this lab, we will use two different playbooks to manage configuration for Spine and Leaf switches. We will use role structure and manage the two plays into single playbook. A role directory structure contains several directories of defaults, vars, files, handlers, meta, tasks and templates. In this lab: we will use vars, templates and tasks folders main.yml file in /vars folder contains dictionary of variables for this role main.yml file in /tasks folder contains the Ansible playbook for this role To proceed further with roles in subsequent Tasks: \u2022 Create roles directory in folder LTRDCN-1572 by issuing below commands: [root@rhel7-tools LTRDCN-1572]# cd /root/LTRDCN-1572 [root@rhel7-tools LTRDCN-1572]# mkdir roles This will be used in the subsequent tasks in this lab.","title":"Task 1 - Prepare Ansible node"},{"location":"Task1-ansible-node/#step-1-connect-to-lab-using-anyconnect-vpn","text":"You will connect to dcloud-rtp-anyconnect.cisco.com using Cisco VPN AnyConnect client, as shown in below picture, with the username and password provided by the lab admin. Note: lab admin will furnish the credentials information to the participant. If you don't have this information please ask the lab speakers.","title":"Step 1: Connect to lab using anyconnect VPN"},{"location":"Task1-ansible-node/#step-2-enter-vpn-credentials","text":"After prompted for credentials, use the credentials provided by the lab admin. \u2022 Below is an example of user logging into a reference POD: Hit accept when the prompt appears to accept the VPN connection login","title":"Step 2: Enter VPN credentials"},{"location":"Task1-ansible-node/#step-3-rdp-to-workstation","text":"In this step, you will connect to the workstation with RDP client on your machines. Use below details for this RDP session: Workstation: 198.18.133.36 Username: dcloud\\demouser Password: C1sco12345 Below screenshot is only an example for this RDP connection:","title":"Step 3: RDP to workstation"},{"location":"Task1-ansible-node/#step-4-mtputty","text":"Once you have the RDP session to the remote workstation, then you will use MTputty client to connect to all devices in this lab. MTputty is already installed on the Desktop of the workstation where you connected using RDP. Run this application by clicking on the icon on the desktop:","title":"Step 4: MTputty"},{"location":"Task1-ansible-node/#step-5-ssh-into-ansible-node","text":"SSH into Ansible node (198.18.134.150) by double clicking the Ansible icon on the left pan with username root and password C1sco12345","title":"Step 5: SSH into Ansible node"},{"location":"Task1-ansible-node/#step-6-verify-python","text":"Once successfully SSH into the ansible node, the very first thing we are going to do after logging into Ansible server is verify the python version by running python --version command - as shown below: [root@rhel7-tools ~]# python --version It is an important step as we need minimum 2.7.5 version of python in order to install some features for ansbile. The output of above command confirms this version. Ansible can be run from any machine with Python 2 (versions 2.6 or 2.7) or Python 3 (versions 3.5 and higher) installed.","title":"Step 6: Verify Python"},{"location":"Task1-ansible-node/#step-7-install-pip","text":"After verifying we have the minimum version of python installed, we are now going to Install PIP python package using easy_install pip command as shown below: [root@rhel7-tools ~]# easy_install pip Below screenshot shows the execution of above command: Next, update pip to latest version by executing command pip install --upgrade pip as shown below: [root@rhel7-tools ~]# pip install --upgrade pip Note: you can ignore the warning related to python version. Below screenshot shows the exection of above command: After installing PIP package, we are going to add the relevant packages that are needed for this Ansible based VXLAN lab. Below are the packages required for this lab. Paramiko PyYAML Jinj2 Httplib2 Run the command pip install paramiko PyYAML jinja2 httplib2 , as shown below, to install these packages: [root@rhel7-tools ~]# pip install paramiko PyYAML jinja2 httplib2 Below screenshot shows the execution of above command: As a final step, we are going to install Ansible on this RHEL. Once the install is initiated by using pip install ansible==2.8.0b1 command (as shown below). It may take few minutes for it to download and install. [root@rhel7-tools ~]# pip install ansible==2.8.0b1 Below screenshot shows the execution of above command:","title":"Step 7: Install PIP"},{"location":"Task1-ansible-node/#step-8-verify-ansible","text":"After installation is complete, check Ansible version by executing command ansible --version , as shown below: [root@rhel7-tools ~]# ansible --version Below screenshot shows the execution of above command:","title":"Step 8: Verify Ansible"},{"location":"Task1-ansible-node/#step-9-create-ansible-inventory","text":"Now, we are going to create inventory, host variables and Configuration file. This is important as Ansible works against multiple systems in the system by selecting portions of systems listed in Ansible inventory. Similarly, configuration settings in Ansible are adjustable via configuration file. Create folder named LTRDCN-1572 as working environment and verify that it\u2019s empty: [root@rhel7-tools ~]# mkdir LTRDCN-1572 && cd LTRDCN-1572 [root@rhel7-tools LTRDCN-1572]# ls Below screenshot shows the execution of above command: Next: Create Ansible inventory file to include Spine and Leaf switches. By default Ansible has inventory file saved in location /etc/ansible/hosts. In this lab we will create hosts file in the working environment. This file is created from ansible host prompt using cat command. Note: you should copy and paste the complete section below i.e., starting from cat till EOF (also shown in subsequent screenshot): cat << EOF >> hosts #define global variables, groups and host variables [all:vars] ansible_connection = local user=admin pwd=C1sco12345 gather_fact=no [jinja2_spine] 198.18.4.202 [jinja2_leaf] 198.18.4.104 [spine] 198.18.4.201 [leaf] 198.18.4.101 198.18.4.103 [server] 198.18.134.50 eth1=172.21.140.10 gw=172.21.140.1 198.18.134.52 eth1=172.21.140.11 gw=172.21.140.1 198.18.134.53 eth1=172.21.141.11 gw=172.21.141.1 EOF Below screenshot shows the output of above command: Now you may verify the content of this file using below command: [root@rhel7-tools LTRDCN-1572]# more hosts Below screenshot shows the execution of above command: Create Ansible config (ansible.cfg) using the same steps as above. Note: you should copy and paste the complete section below i.e., starting from cat till EOF (as shown in below screenshot): cat << EOF >> ansible.cfg [defaults] inventory = hosts host_key_checking = false record_host_key = true stdout_callback = debug deprecation_warnings = False EOF Now you may verify the content of this file using below command: [root@rhel7-tools LTRDCN-1572]# more ansible.cfg Below screenshot shows the output of above commands: Do ls to verify the file that you just created under project folder LTRDCN-1572. Below screenshot shows the execution of above command: Create host variable folder named host_vars in folder LTRDCN-1572 by using below command. In this lab, we will use host_vars file to define the variables for various hosts (in next bullets): [root@rhel7-tools LTRDCN-1572]# mkdir host_vars && cd host_vars Create host variable file for each host in inventory by using the cat command. The variable file for a switch is created using the below cat command. Note: you can copy and paste starting from cat till EOF (as shown in below screenshot). Note: the spaces in the file are important so do not remove those: cat << EOF >> 198.18.4.101.yml --- hostname: leaf_1 loopback0: 192.168.0.8 loopback1: 192.168.0.18 router_id: 192.168.0.8 EOF Below screenshot shows the execution of above command: Now you may verify the content of this file using more 198.18.4.101.yml as shown below: [root@rhel7-tools LTRDCN-1572]# more 198.18.4.101.yml Below screenshot shows the execution of above command: Create a new host variable file for next host in inventory. The variable file for a switch is created using the below cat command. Note: you can copy and paste starting from cat till EOF (as shown in below screenshot). Note: the spaces in the file are important so do not remove those: cat << EOF >> 198.18.4.103.yml --- hostname: leaf_3 loopback0: 192.168.0.10 loopback1: 192.168.0.110 router_id: 192.168.0.10 EOF Below screenshot shows the execution of above command: Now you may verify the content of this file using more 198.18.4.103.yml as shown below: [root@rhel7-tools LTRDCN-1572]# more 198.18.4.103.yml Below screenshot shows the execution of above command: Create a new host variable file for next host in inventory. The variable file for a switch is created using the below cat command. Note: you can copy and paste starting from cat till EOF (as shown in below screenshot). Note: the spaces in the file are important so do not remove those: cat << EOF >> 198.18.4.104.yml --- hostname: leaf_4 loopback0: 192.168.0.11 loopback1: 192.168.0.111 router_id: 192.168.0.11 EOF Now you may verify the content of this file using more 198.18.4.104.yml as shown below: [root@rhel7-tools LTRDCN-1572]# more 198.18.4.104.yml Below screenshot shows the execution of above commands: Create a new host variable file for next host in inventory. The variable file for a switch is created using the below cat command. Note: you can copy and paste starting from cat till EOF (as shown in below screenshot). Note: the spaces in the file are important so do not remove those: cat << EOF >> 198.18.4.201.yml --- hostname: spine-1 loopback0: 192.168.0.6 loopback1: 192.168.0.100 router_id: 192.168.0.6 EOF Now you may verify the content of this file using more 198.18.4.201.yml as shown below: [root@rhel7-tools LTRDCN-1572]# more 198.18.4.201.yml Below screenshot shows the execution of above commands: Create a new host variable file for next host in inventory. The variable file for a switch is created using the below cat command. Note: you can copy and paste starting from cat till EOF (as shown in below screenshot). Note: the spaces in the file are important so do not remove those: cat << EOF >> 198.18.4.202.yml --- hostname: spine-2 loopback0: 192.168.0.7 loopback1: 192.168.0.100 router_id: 192.168.0.7 EOF Now you may verify the content of this file using more 198.18.4.202.yml as shown below: [root@rhel7-tools LTRDCN-1572]# more 198.18.4.202.yml Below screenshot shows the execution of above commands:","title":"Step 9: Create Ansible Inventory"},{"location":"Task1-ansible-node/#step-10-ansible-role-structure","text":"Role is very useful technique to manage a set of playbooks in Ansible. In this lab, we will use two different playbooks to manage configuration for Spine and Leaf switches. We will use role structure and manage the two plays into single playbook. A role directory structure contains several directories of defaults, vars, files, handlers, meta, tasks and templates. In this lab: we will use vars, templates and tasks folders main.yml file in /vars folder contains dictionary of variables for this role main.yml file in /tasks folder contains the Ansible playbook for this role To proceed further with roles in subsequent Tasks: \u2022 Create roles directory in folder LTRDCN-1572 by issuing below commands: [root@rhel7-tools LTRDCN-1572]# cd /root/LTRDCN-1572 [root@rhel7-tools LTRDCN-1572]# mkdir roles This will be used in the subsequent tasks in this lab.","title":"Step 10: Ansible role structure"},{"location":"appendixA-F5/","text":"Appendix A: L4-L7 insertion In this section, you will insert F5 BIG-IP load balancer into the fabric. - First send out a http request from server-4 to a VIP, and you will notice no http service enabled on VIP. - Run the playbook to enable http service on the VIP with server-1, server-2, server-3 and server-4 in the server pool. - After successful execute the playbook, you will notice http request to VIP is load balanced across three servers. Step 1: Power on F5 Virtual Machine open VMware vSphere Client to login ESXi (198.18.133.33) using crendentials of root/C1sco12345 Open up the VM under host 198.18.133.33 , and power on VM named F5_LTM as shown below: Step 2: Install Ansible pre-reqs Switch to MTPuTTY install prerequisites packages on Ansible node by using below pip install ... commands ( Note : the bold commands need to be executed): [root@rhel7-tools LTRDCN-1572]# pip install f5-sdk [root@rhel7-tools LTRDCN-1572]# pip install setuptools --upgrade [root@rhel7-tools LTRDCN-1572]# pip install bigsuds [root@rhel7-tools LTRDCN-1572]# pip install netaddr Step 3: Check Virtual IP using curl Switch to \u2018MTPuTTY\u2019 and ssh into server-4. Then send http request to Virtual IP (VIP) 172.21.140.100 from server-4 using curl http://172.21.140.100 command Below output shows the execution of above command i.e., HTTP request to VIP should fail: [root@server-4 ~]# curl http://172.21.140.100 curl: (7) couldn't connect to host` [root@server-4 ~]# Step 4: Switch to Atom , create new file name \u2018f5ltm.yml\u2019 under project folder LTRDCN-1572 . Enter below data in this new file (named \u2018f5ltm.yml\u2019 ) Make sure to click File and Save to ftp this data to Ansible server: --- - name: Configurating BIG-IP hosts: localhost gather_facts: false vars: provider: password: admin server: 198.18.4.10 user: admin validate_certs: False tasks: - name: Configure server facing port to L2 nxos_interface: interface: eth1/4 mode: layer2 username: \"{{ user }}\" password: \"{{ pwd }}\" transport: nxapi host: \"198.18.4.104\" - name: Configure VLAN for F5 port nxos_switchport: interface: eth1/4 mode: access access_vlan: 140 username: \"{{ user }}\" password: \"{{ pwd }}\" transport: nxapi host: \"198.18.4.104\" - name: Configure VLANs on the BIG-IP bigip_vlan: name: \"External\" tag: \"140\" untagged_interface: \"1.1\" provider: \"{{ provider }}\" - name: Configure SELF-IPs on the BIG-IP bigip_selfip: name: \"172.21.140.50\" address: \"172.21.140.50\" netmask: \"255.255.255.0\" vlan: \"External\" allow_service: \"default\" provider: \"{{ provider }}\" - name: Create static route bigip_static_route: provider: \"{{ provider }}\" destination: 0.0.0.0 netmask: 0.0.0.0 gateway_address: 172.21.140.1 name: \"defult\" - name: Create nodes bigip_node: provider: \"{{ provider }}\" host: \"{{item}}\" name: \"{{item}}\" with_items: - 172.21.140.10 - 172.21.140.11 - 172.21.141.10 - 172.21.141.11 - name: Create pool bigip_pool: provider: \"{{ provider }}\" name: \"web-pool\" lb_method: \"round-robin\" monitors: \"/Common/http\" monitor_type: \"and_list\" - name: Add Pool members bigip_pool_member: provider: \"{{ provider }}\" name: \"{{item}}\" host: \"{{item}}\" port: \"80\" pool: \"web-pool\" with_items: - 172.21.140.10 - 172.21.140.11 - 172.21.141.10 - 172.21.141.11 - name: Add Virtual Server bigip_virtual_server: provider: \"{{ provider }}\" name: \"http-virtualserver\" destination: \"172.21.140.100\" port: \"80\" enabled_vlans: \"ALL\" all_profiles: - http pool: \"web-pool\" snat: \"Automap\" Step 5: Run playbook On the Ansible node (using MTputty SSH), run playbook f5ltm.yml to provision VIP (172.21.140.100) on F5 and to also put all four servers into the server pool using ansible-playbook f5ltm.yml command. Below shows the output of above command: Step 6: Check load balancer Switch to \u2018MTPuTTY\u2019 and login to server-4 , Run curl http://172.21.140.100 command multiple times, Note that the request is load balanced to differert servers in the sever pool The execution and output of above command (when its run multiple times) is shown below: [root@server-4 ~]# curl http://172.21.140.100 <html><body><h1>It works!</h1> <p> Server-1 172.21.140.10 </p> <p>This is the default web page for this server.</p> <p>The web server software is running but no content has been added, yet.</p> </body></html> [root@server-4 ~]# curl http://172.21.140.100 <html><body><h1>It works!</h1> <p> Server-3 172.21.140.11 </p> <p>This is the default web page for this server.</p> <p>The web server software is running but no content has been added, yet.</p> </body></html> [root@server-4 ~]# curl http://172.21.140.100 <html><body><h1>It works!</h1> <p> Server-4 172.21.141.11 </p> <p>This is the default web page for this server.</p> <p>The web server software is running but no content has been added, yet.</p> </body></html> This concludes Appendix A using ansible to insert F5 (as load balancer) in the DC Fabric. Well done!","title":"Appendix A - F5"},{"location":"appendixA-F5/#appendix-a-l4-l7-insertion","text":"In this section, you will insert F5 BIG-IP load balancer into the fabric. - First send out a http request from server-4 to a VIP, and you will notice no http service enabled on VIP. - Run the playbook to enable http service on the VIP with server-1, server-2, server-3 and server-4 in the server pool. - After successful execute the playbook, you will notice http request to VIP is load balanced across three servers.","title":"Appendix A: L4-L7 insertion"},{"location":"appendixA-F5/#step-1-power-on-f5-virtual-machine","text":"open VMware vSphere Client to login ESXi (198.18.133.33) using crendentials of root/C1sco12345 Open up the VM under host 198.18.133.33 , and power on VM named F5_LTM as shown below:","title":"Step 1: Power on F5 Virtual Machine"},{"location":"appendixA-F5/#step-2-install-ansible-pre-reqs","text":"Switch to MTPuTTY install prerequisites packages on Ansible node by using below pip install ... commands ( Note : the bold commands need to be executed): [root@rhel7-tools LTRDCN-1572]# pip install f5-sdk [root@rhel7-tools LTRDCN-1572]# pip install setuptools --upgrade [root@rhel7-tools LTRDCN-1572]# pip install bigsuds [root@rhel7-tools LTRDCN-1572]# pip install netaddr","title":"Step 2: Install Ansible pre-reqs"},{"location":"appendixA-F5/#step-3-check-virtual-ip-using-curl","text":"Switch to \u2018MTPuTTY\u2019 and ssh into server-4. Then send http request to Virtual IP (VIP) 172.21.140.100 from server-4 using curl http://172.21.140.100 command Below output shows the execution of above command i.e., HTTP request to VIP should fail: [root@server-4 ~]# curl http://172.21.140.100 curl: (7) couldn't connect to host` [root@server-4 ~]#","title":"Step 3: Check Virtual IP using curl"},{"location":"appendixA-F5/#step-4","text":"Switch to Atom , create new file name \u2018f5ltm.yml\u2019 under project folder LTRDCN-1572 . Enter below data in this new file (named \u2018f5ltm.yml\u2019 ) Make sure to click File and Save to ftp this data to Ansible server: --- - name: Configurating BIG-IP hosts: localhost gather_facts: false vars: provider: password: admin server: 198.18.4.10 user: admin validate_certs: False tasks: - name: Configure server facing port to L2 nxos_interface: interface: eth1/4 mode: layer2 username: \"{{ user }}\" password: \"{{ pwd }}\" transport: nxapi host: \"198.18.4.104\" - name: Configure VLAN for F5 port nxos_switchport: interface: eth1/4 mode: access access_vlan: 140 username: \"{{ user }}\" password: \"{{ pwd }}\" transport: nxapi host: \"198.18.4.104\" - name: Configure VLANs on the BIG-IP bigip_vlan: name: \"External\" tag: \"140\" untagged_interface: \"1.1\" provider: \"{{ provider }}\" - name: Configure SELF-IPs on the BIG-IP bigip_selfip: name: \"172.21.140.50\" address: \"172.21.140.50\" netmask: \"255.255.255.0\" vlan: \"External\" allow_service: \"default\" provider: \"{{ provider }}\" - name: Create static route bigip_static_route: provider: \"{{ provider }}\" destination: 0.0.0.0 netmask: 0.0.0.0 gateway_address: 172.21.140.1 name: \"defult\" - name: Create nodes bigip_node: provider: \"{{ provider }}\" host: \"{{item}}\" name: \"{{item}}\" with_items: - 172.21.140.10 - 172.21.140.11 - 172.21.141.10 - 172.21.141.11 - name: Create pool bigip_pool: provider: \"{{ provider }}\" name: \"web-pool\" lb_method: \"round-robin\" monitors: \"/Common/http\" monitor_type: \"and_list\" - name: Add Pool members bigip_pool_member: provider: \"{{ provider }}\" name: \"{{item}}\" host: \"{{item}}\" port: \"80\" pool: \"web-pool\" with_items: - 172.21.140.10 - 172.21.140.11 - 172.21.141.10 - 172.21.141.11 - name: Add Virtual Server bigip_virtual_server: provider: \"{{ provider }}\" name: \"http-virtualserver\" destination: \"172.21.140.100\" port: \"80\" enabled_vlans: \"ALL\" all_profiles: - http pool: \"web-pool\" snat: \"Automap\"","title":"Step 4:"},{"location":"appendixA-F5/#step-5-run-playbook","text":"On the Ansible node (using MTputty SSH), run playbook f5ltm.yml to provision VIP (172.21.140.100) on F5 and to also put all four servers into the server pool using ansible-playbook f5ltm.yml command. Below shows the output of above command:","title":"Step 5: Run playbook"},{"location":"appendixA-F5/#step-6-check-load-balancer","text":"Switch to \u2018MTPuTTY\u2019 and login to server-4 , Run curl http://172.21.140.100 command multiple times, Note that the request is load balanced to differert servers in the sever pool The execution and output of above command (when its run multiple times) is shown below: [root@server-4 ~]# curl http://172.21.140.100 <html><body><h1>It works!</h1> <p> Server-1 172.21.140.10 </p> <p>This is the default web page for this server.</p> <p>The web server software is running but no content has been added, yet.</p> </body></html> [root@server-4 ~]# curl http://172.21.140.100 <html><body><h1>It works!</h1> <p> Server-3 172.21.140.11 </p> <p>This is the default web page for this server.</p> <p>The web server software is running but no content has been added, yet.</p> </body></html> [root@server-4 ~]# curl http://172.21.140.100 <html><body><h1>It works!</h1> <p> Server-4 172.21.141.11 </p> <p>This is the default web page for this server.</p> <p>The web server software is running but no content has been added, yet.</p> </body></html> This concludes Appendix A using ansible to insert F5 (as load balancer) in the DC Fabric. Well done!","title":"Step 6: Check load balancer"},{"location":"appendixB-POAP/","text":"Appendix B: Day 0 automation using POAP In this section, you will deploy Power On Auto Provisioning (POAP) for leaf4 in DCNM. mgmt Loopback0 Loopback1 Eth1/1 Eth1/2 198.18.4.104 198.168.0.11 198.168.0.111 10.0.128.2 10.0.128.18 Step 1: Open Google Chrome and login Cisco DCNM (https://198.18.134.200) using username/password admin/C1sco12345 Step 2: On the left side menu, click Configuration > Deploy > POAP Step 3: Click DHCP Scopes on POAP page Step 4: Click plus sign to add new DHCP scope, name it \u2018vxlan_evpn_leaf\u2019 add DCHP range \u2018198.18.4.100-198.18.4.104\u2019 as IP pool Step 5: click OK to close the DHCP scope window Step 6: Click left side menu Configure > Deploy > POAP Step 7: Click Images and Configuration on the POAP page Step 8: Check the Default_SCP_Repository , click File Browser to verify image nxos.7.0.3.I7.0.154.bin file is in the repository. Step 9: Click left side menu Configure > Deploy > POAP Step 10: Client \u2018template\u2019 under POAP Definitions Step 11: Click import Template from the Templates page Step 12: Locate simple_template.template file on desktop, and open the template. Step 13: Save the template Step 14: Click left side menu Configure > Deploy > POAP Step 15: Click plus sign to add new POAP Definitions Step 16: Check Generate Definition and click next Step 17: Switch to MTPuTTY open connection to switch leaf-4 , type in command \u2018show module\u2019 and write down the Serial-Num Note: The Serial-Num from your output might be different Put the Serial Number (9VSCMHEMJ69) for switch leaf-4 in \u2018Switches\u2019 field. Fill in other information on the page as following. Use drop down list to fill in rest information Switch Type: N9K Image Server: Default_SCP_Repository Switch Username: admin Switch Password: C1sco12345 Step 18: Click Next and select simple_template from template drop down list, and fill in other information as below Switch Name : leaf-4 Administrative Username: Admin Administrative Password: c1sco12345 Management IP: 198.18.4.104 Management Prefix: 24 Default Gateway: 198.18.4.1 Console timeout: 0 Console Speed : 9600 Step 19: Select switch leaf-4 with Serial Number \u201891AQACU3U3UH9\u2019 Click Next Step 20: Click Preview CLI as shown below Then click Finish Step 21: select switch leaf-4 , Click Write Erase and Reload to delte leaf-4 configuration and kick off the POAP process Click Continue on the popup warning Step 22: Connect to Leaf-4 console port from MTPuTTY, watch the POAP process If the POAP scripts fail, verify the S/N in console. If the S/N doesn\u2019t match what you configured in POAP definition, you need to configure new POAP definition matches the S/N. Step 23: Login leaf-4 to verify running configuration after POAP is completed POAP will take 20 mins to bootup; up to this point, you have completed all tasks. Congratulation! You have completed the whole lab.","title":"appendixB POAP"},{"location":"appendixB-POAP/#appendix-b-day-0-automation-using-poap","text":"In this section, you will deploy Power On Auto Provisioning (POAP) for leaf4 in DCNM. mgmt Loopback0 Loopback1 Eth1/1 Eth1/2 198.18.4.104 198.168.0.11 198.168.0.111 10.0.128.2 10.0.128.18","title":"Appendix B: Day 0 automation using POAP"},{"location":"appendixB-POAP/#step-1","text":"Open Google Chrome and login Cisco DCNM (https://198.18.134.200) using username/password admin/C1sco12345","title":"Step 1:"},{"location":"appendixB-POAP/#step-2","text":"On the left side menu, click Configuration > Deploy > POAP","title":"Step 2:"},{"location":"appendixB-POAP/#step-3","text":"Click DHCP Scopes on POAP page","title":"Step 3:"},{"location":"appendixB-POAP/#step-4","text":"Click plus sign to add new DHCP scope, name it \u2018vxlan_evpn_leaf\u2019 add DCHP range \u2018198.18.4.100-198.18.4.104\u2019 as IP pool","title":"Step 4:"},{"location":"appendixB-POAP/#step-5","text":"click OK to close the DHCP scope window","title":"Step 5:"},{"location":"appendixB-POAP/#step-6","text":"Click left side menu Configure > Deploy > POAP","title":"Step 6:"},{"location":"appendixB-POAP/#step-7","text":"Click Images and Configuration on the POAP page","title":"Step 7:"},{"location":"appendixB-POAP/#step-8","text":"Check the Default_SCP_Repository , click File Browser to verify image nxos.7.0.3.I7.0.154.bin file is in the repository.","title":"Step 8:"},{"location":"appendixB-POAP/#step-9","text":"Click left side menu Configure > Deploy > POAP","title":"Step 9:"},{"location":"appendixB-POAP/#step-10","text":"Client \u2018template\u2019 under POAP Definitions","title":"Step 10:"},{"location":"appendixB-POAP/#step-11","text":"Click import Template from the Templates page","title":"Step 11:"},{"location":"appendixB-POAP/#step-12","text":"Locate simple_template.template file on desktop, and open the template.","title":"Step 12:"},{"location":"appendixB-POAP/#step-13","text":"Save the template","title":"Step 13:"},{"location":"appendixB-POAP/#step-14","text":"Click left side menu Configure > Deploy > POAP","title":"Step 14:"},{"location":"appendixB-POAP/#step-15","text":"Click plus sign to add new POAP Definitions","title":"Step 15:"},{"location":"appendixB-POAP/#step-16","text":"Check Generate Definition and click next","title":"Step 16:"},{"location":"appendixB-POAP/#step-17","text":"Switch to MTPuTTY open connection to switch leaf-4 , type in command \u2018show module\u2019 and write down the Serial-Num Note: The Serial-Num from your output might be different Put the Serial Number (9VSCMHEMJ69) for switch leaf-4 in \u2018Switches\u2019 field. Fill in other information on the page as following. Use drop down list to fill in rest information Switch Type: N9K Image Server: Default_SCP_Repository Switch Username: admin Switch Password: C1sco12345","title":"Step 17:"},{"location":"appendixB-POAP/#step-18","text":"Click Next and select simple_template from template drop down list, and fill in other information as below Switch Name : leaf-4 Administrative Username: Admin Administrative Password: c1sco12345 Management IP: 198.18.4.104 Management Prefix: 24 Default Gateway: 198.18.4.1 Console timeout: 0 Console Speed : 9600","title":"Step 18:"},{"location":"appendixB-POAP/#step-19","text":"Select switch leaf-4 with Serial Number \u201891AQACU3U3UH9\u2019 Click Next","title":"Step 19:"},{"location":"appendixB-POAP/#step-20","text":"Click Preview CLI as shown below Then click Finish","title":"Step 20:"},{"location":"appendixB-POAP/#step-21","text":"select switch leaf-4 , Click Write Erase and Reload to delte leaf-4 configuration and kick off the POAP process Click Continue on the popup warning","title":"Step 21:"},{"location":"appendixB-POAP/#step-22","text":"Connect to Leaf-4 console port from MTPuTTY, watch the POAP process If the POAP scripts fail, verify the S/N in console. If the S/N doesn\u2019t match what you configured in POAP definition, you need to configure new POAP definition matches the S/N.","title":"Step 22:"},{"location":"appendixB-POAP/#step-23","text":"Login leaf-4 to verify running configuration after POAP is completed POAP will take 20 mins to bootup; up to this point, you have completed all tasks.","title":"Step 23:"},{"location":"appendixB-POAP/#congratulation-you-have-completed-the-whole-lab","text":"","title":"Congratulation! You have completed the whole lab."},{"location":"appendixB-Upgrade/","text":"Appendix B: Software compliance check and remediation In this section, we will run software version compliance check using Ansible. For fabric switch that is not running on standard software version, we will perform software upgrade and bring all fabric switches into the standard version. In this playbook, we will use \u201cnxos_facts\u201d to find the software version on each fabric switch. Then we will compare with standard software version, 7.0(3)I7(4) in this lab. For fabric switch that is not running on standard version, the playbook will upgrade and reboot the switch. The playbook will use \u201cnxos_file_copy\u201d module to copy image from remote repository to bootflash. A bug fix is introduced in Ansible 2.9 and upward to avoid timeout issue when coping large file, so fist we will upgrade ansible to 2.9.1 On Ansible server execute command pip install ansible==2.9.1 to upgrade Ansible to 2.9.1 release: [root@rhel7-tools LTRDCN-1572]# pip install ansible==2.9.1 On Atom, open up the project folder LTRDCN-1572 and create new file under this folder (\u201cLTRDCN-1572\u201d). Name the new file \u201ccode_upgrade.yml\u201d. and add below content (you may copy and paste): --- #Appendix code upgrade - hosts: spine,leaf vars: - standard: 7.0(3)I7(4) - image_file: nxos.7.0.3.I7.4.bin - ansible_connection: network_cli - ansible_network_os: nxos - ansible_user: \"{{ user }}\" - ansible_password: \"{{ pwd }}\" - ansible_connect_timeout: 120 tasks: - name: \"software complaince check\" nxos_facts: gather_subset: all - name: \"change to standard code\" block: - debug: msg=\"{{ansible_net_hostname}} is not running standard {{standard}}\" - nxos_feature: feature: scp-server state: enabled - name: \"upload image file\" nxos_file_copy: file_pull: True file_pull_timeout: 1200 remote_file: \"/home/admin/downloads/{{image_file}}\" remote_scp_server: \"198.18.4.150\" remote_scp_server_user: \"root\" remote_scp_server_password: \"C1sco12345\" - name: \"change boot statement\" nxos_config: lines: boot nxos bootflash:{{image_file}} save_when: modified - name: \"reload switch\" ios_command: commands: - command: reload prompt: '(y/n)?' answer: 'y' username: \"{{ user }}\" password: \"{{ pwd }}\" when: ansible_net_version != standard rescue: - debug: msg: \"{{ansible_net_hostname}} is reloading\" - name: Wait For Device To Come Back Up wait_for: port: 22 state: started timeout: 900 delay: 60 host: \"{{ inventory_hostname }}\" always: - debug: msg: \"All devices are running {{standard}}\" On the Ansible server, run the playbook for software compliance check and code upgrade by issuing the ansible-playbook code_upgrade.yml command as shown below: [root@rhel7-tools LTRDCN-1572]# ansible-playbook code_upgrade.yml Note: It is expected to see timeout error message when playbook reloads the switch. Below screenshot shows the output of above command: Switch will take 20 mins to bootup; up to this point, you have completed all tasks.* Congratulations! You have completed the whole lab including the Optional (Appendix) sections. Well done!","title":"Appendix B - Upgrade"},{"location":"appendixB-Upgrade/#appendix-b-software-compliance-check-and-remediation","text":"In this section, we will run software version compliance check using Ansible. For fabric switch that is not running on standard software version, we will perform software upgrade and bring all fabric switches into the standard version. In this playbook, we will use \u201cnxos_facts\u201d to find the software version on each fabric switch. Then we will compare with standard software version, 7.0(3)I7(4) in this lab. For fabric switch that is not running on standard version, the playbook will upgrade and reboot the switch. The playbook will use \u201cnxos_file_copy\u201d module to copy image from remote repository to bootflash. A bug fix is introduced in Ansible 2.9 and upward to avoid timeout issue when coping large file, so fist we will upgrade ansible to 2.9.1 On Ansible server execute command pip install ansible==2.9.1 to upgrade Ansible to 2.9.1 release: [root@rhel7-tools LTRDCN-1572]# pip install ansible==2.9.1 On Atom, open up the project folder LTRDCN-1572 and create new file under this folder (\u201cLTRDCN-1572\u201d). Name the new file \u201ccode_upgrade.yml\u201d. and add below content (you may copy and paste): --- #Appendix code upgrade - hosts: spine,leaf vars: - standard: 7.0(3)I7(4) - image_file: nxos.7.0.3.I7.4.bin - ansible_connection: network_cli - ansible_network_os: nxos - ansible_user: \"{{ user }}\" - ansible_password: \"{{ pwd }}\" - ansible_connect_timeout: 120 tasks: - name: \"software complaince check\" nxos_facts: gather_subset: all - name: \"change to standard code\" block: - debug: msg=\"{{ansible_net_hostname}} is not running standard {{standard}}\" - nxos_feature: feature: scp-server state: enabled - name: \"upload image file\" nxos_file_copy: file_pull: True file_pull_timeout: 1200 remote_file: \"/home/admin/downloads/{{image_file}}\" remote_scp_server: \"198.18.4.150\" remote_scp_server_user: \"root\" remote_scp_server_password: \"C1sco12345\" - name: \"change boot statement\" nxos_config: lines: boot nxos bootflash:{{image_file}} save_when: modified - name: \"reload switch\" ios_command: commands: - command: reload prompt: '(y/n)?' answer: 'y' username: \"{{ user }}\" password: \"{{ pwd }}\" when: ansible_net_version != standard rescue: - debug: msg: \"{{ansible_net_hostname}} is reloading\" - name: Wait For Device To Come Back Up wait_for: port: 22 state: started timeout: 900 delay: 60 host: \"{{ inventory_hostname }}\" always: - debug: msg: \"All devices are running {{standard}}\" On the Ansible server, run the playbook for software compliance check and code upgrade by issuing the ansible-playbook code_upgrade.yml command as shown below: [root@rhel7-tools LTRDCN-1572]# ansible-playbook code_upgrade.yml Note: It is expected to see timeout error message when playbook reloads the switch. Below screenshot shows the output of above command: Switch will take 20 mins to bootup; up to this point, you have completed all tasks.*","title":"Appendix B: Software compliance check and remediation"},{"location":"appendixB-Upgrade/#congratulations-you-have-completed-the-whole-lab-including-the-optional-appendix-sections-well-done","text":"","title":"Congratulations! You have completed the whole lab including the Optional (Appendix) sections. Well done!"},{"location":"intro/","text":"VXLAN VXLAN stands for Virtual Extensible Local Area Network. VXLAN is a L2 overlay scheme on top of L3 network or we can say it is a L2 in layer 3 tunnel. It runs over the existing networks and provides the means to stretch the L2 network. Only VMs within the same VXLAN segment can communicate with each other. Each VXLAN segment is identified by a 24 bit segment ID called \u201cVXLAN Network Identifier (VNI)\u201d. This help overcome 4094 VLAN scale limitation and able to extend it to 224 segments. VXLAN uses BGP as its control plane for Overlay. It makes it forwarding decisions at VTEPs (Virtual tunnel end points) for layer-2 and layer-3. Forwarding happens based on MAC or IP learnt via control plane (MP-BGP EVPN) . VXLAN uses IGP, PIM and BGP as its underlay in the fabric. Below are some of the terminologies that will be used in the lab: VNI / VNID \u2013 VXLAN Network Identifier. This replaces VLAN ID VTEP \u2013 VXLAN Tunnel End Point. This is the end point where the box performs VXLAN encap / decap This could be physical HW (Nexus9k) or Virtual (Nexus 1000v, Nexus 9000v) VXLAN Segment - The resulting layer 2 overlay network VXLAN Gateway \u2013 It is a device that forwards traffic between VXLANS. It can be both L2 and L3 forwarding NVE \u2013 Network Virtualization Edge NVE is tunnel interface. It represents VTEP Ansible Ansible is an agentless open source software that can be used for configuration management, deployment and orchestration of deployment. The scripts in Ansible are called playbooks; playbook is in YAML format that was desgiened to be easy for humans to read and write. Playbooks include one or more plays, each play include one or more tasks. Each task is associated with one module, which is what gets executed in the playbook. Modules are python scripts that ship with Ansible installation. During the lab, you will be introduced to multiple NXOS modules and ansible template module. You can find all Ansible modules documentation at below url: http://docs.ansible.com/ansible/latest/list_of_all_modules.html Below are some of the terminologies that will be used in the lab: Host : remote machines that Ansible manages Group : several hosts that can be configured together and share common verables Inventory : file descripts hosts and groups in Ansible. Variable : names of value (int, str, dic, list) referenced in playbook or template YAML : data format for Playbook or Variables in Ansible Playbook : the script to orchestrate, automate, deploy system in Ansible. One playbook can include multiple plays. Roles : group of tasks, templates to implement specific behavior Jinja2 : a Python based tempting language About this lab As a standardized overlay technology, multiple vendors have adopted VXLAN as datacenter solution to provide scalability and allow layer 2 across IP network. MP-BPG EVPN as VXLAN control plane protocol provides a robust scalable solution to overcome the limitation in VXLAN flood and learn mode. As an open source automation tool, Ansible provides the same framework for network administrators to automate network infrastructure as the rest IT organization. This lab demostates the possibility of using Ansible to automate datacenter VXLAN fabric day 1 provisiong and day 2 operations. Lab Flow Lab guide will walk the attendees through the below activities: 1. Ansible installation 2. Ansible playbook 3. Day 1 automation using Ansible 4. Day 2 automation using Ansible 5. Day 0 automation 6. L4-L7 Service insertion Lab Access Below table provides the IP addresses and credentials for the devices used in this lab: Device SSH or Console (C) Credentials Spine-1 C: 198.18.133.33:1030 .... SSH: 198.18.4.201 admin/C1sco12345 Spine-2 C: 198.18.133.33:1040 .... SSH: 198.18.4.202 admin/C1sco12345 Leaf-1 C: 198.18.133.33:1050 .... SSH: 198.18.4.101 admin/C1sco12345 Leaf-3 C: 198.18.133.33:1070 .... SSH: 198.18.4.103 admin/C1sco12345 Leaf-4 C: 198.18.1333.33:1080 ... SSH: 198.18.4.104 admin/C1sco12345 Server-1 SSH: 198.18.134.50 root/C1sco12345 Server-3 SSH: 198.18.134.52 root/C1sco12345 Server-4 SSH: 198.18.134.53 root/C1sco12345 Ansible Server 198.18.134.150 root/C1sco12345 Remote Workstation 198.18.133.36 demouser/C1sco12345 Lab topology Below picture shows the lab topology:","title":"Introduction"},{"location":"intro/#vxlan","text":"VXLAN stands for Virtual Extensible Local Area Network. VXLAN is a L2 overlay scheme on top of L3 network or we can say it is a L2 in layer 3 tunnel. It runs over the existing networks and provides the means to stretch the L2 network. Only VMs within the same VXLAN segment can communicate with each other. Each VXLAN segment is identified by a 24 bit segment ID called \u201cVXLAN Network Identifier (VNI)\u201d. This help overcome 4094 VLAN scale limitation and able to extend it to 224 segments. VXLAN uses BGP as its control plane for Overlay. It makes it forwarding decisions at VTEPs (Virtual tunnel end points) for layer-2 and layer-3. Forwarding happens based on MAC or IP learnt via control plane (MP-BGP EVPN) . VXLAN uses IGP, PIM and BGP as its underlay in the fabric. Below are some of the terminologies that will be used in the lab: VNI / VNID \u2013 VXLAN Network Identifier. This replaces VLAN ID VTEP \u2013 VXLAN Tunnel End Point. This is the end point where the box performs VXLAN encap / decap This could be physical HW (Nexus9k) or Virtual (Nexus 1000v, Nexus 9000v) VXLAN Segment - The resulting layer 2 overlay network VXLAN Gateway \u2013 It is a device that forwards traffic between VXLANS. It can be both L2 and L3 forwarding NVE \u2013 Network Virtualization Edge NVE is tunnel interface. It represents VTEP","title":"VXLAN"},{"location":"intro/#ansible","text":"Ansible is an agentless open source software that can be used for configuration management, deployment and orchestration of deployment. The scripts in Ansible are called playbooks; playbook is in YAML format that was desgiened to be easy for humans to read and write. Playbooks include one or more plays, each play include one or more tasks. Each task is associated with one module, which is what gets executed in the playbook. Modules are python scripts that ship with Ansible installation. During the lab, you will be introduced to multiple NXOS modules and ansible template module. You can find all Ansible modules documentation at below url: http://docs.ansible.com/ansible/latest/list_of_all_modules.html Below are some of the terminologies that will be used in the lab: Host : remote machines that Ansible manages Group : several hosts that can be configured together and share common verables Inventory : file descripts hosts and groups in Ansible. Variable : names of value (int, str, dic, list) referenced in playbook or template YAML : data format for Playbook or Variables in Ansible Playbook : the script to orchestrate, automate, deploy system in Ansible. One playbook can include multiple plays. Roles : group of tasks, templates to implement specific behavior Jinja2 : a Python based tempting language","title":"Ansible"},{"location":"intro/#about-this-lab","text":"As a standardized overlay technology, multiple vendors have adopted VXLAN as datacenter solution to provide scalability and allow layer 2 across IP network. MP-BPG EVPN as VXLAN control plane protocol provides a robust scalable solution to overcome the limitation in VXLAN flood and learn mode. As an open source automation tool, Ansible provides the same framework for network administrators to automate network infrastructure as the rest IT organization. This lab demostates the possibility of using Ansible to automate datacenter VXLAN fabric day 1 provisiong and day 2 operations.","title":"About this lab"},{"location":"intro/#lab-flow","text":"Lab guide will walk the attendees through the below activities: 1. Ansible installation 2. Ansible playbook 3. Day 1 automation using Ansible 4. Day 2 automation using Ansible 5. Day 0 automation 6. L4-L7 Service insertion","title":"Lab Flow"},{"location":"intro/#lab-access","text":"Below table provides the IP addresses and credentials for the devices used in this lab: Device SSH or Console (C) Credentials Spine-1 C: 198.18.133.33:1030 .... SSH: 198.18.4.201 admin/C1sco12345 Spine-2 C: 198.18.133.33:1040 .... SSH: 198.18.4.202 admin/C1sco12345 Leaf-1 C: 198.18.133.33:1050 .... SSH: 198.18.4.101 admin/C1sco12345 Leaf-3 C: 198.18.133.33:1070 .... SSH: 198.18.4.103 admin/C1sco12345 Leaf-4 C: 198.18.1333.33:1080 ... SSH: 198.18.4.104 admin/C1sco12345 Server-1 SSH: 198.18.134.50 root/C1sco12345 Server-3 SSH: 198.18.134.52 root/C1sco12345 Server-4 SSH: 198.18.134.53 root/C1sco12345 Ansible Server 198.18.134.150 root/C1sco12345 Remote Workstation 198.18.133.36 demouser/C1sco12345","title":"Lab Access"},{"location":"intro/#lab-topology","text":"Below picture shows the lab topology:","title":"Lab topology"},{"location":"task2-first-ansible/","text":"In this section, we will create the first Ansible Playbook. In the playbook, we will configure VLANs on leaf switches, and assign VLANs to the server facing port. You will learn create variables inside playbook, learn simple loop using \u201c with_items \u201d, learn simple logical using \u201c when \u201d and learn \u201c tags \u201d to isolate tasks from whole playbook. Step 1: Using \"Atom\" text Editor Open \u201cAtom\u201d text editor by double click the icon on desktop. Atom is the recommended text editor for this lab: After opening ATOM, Click No, Never to the \u201cRegister as default atom:// URI handler\u201d message as show below: As an optional step: on ATOM you may close the Welcome, Welcome Guide, Telemetry Consent tabs. Step 2: Atom folder After opening ATOM, there should be a folder in the left pane named \u201cLTRDCN-1572.\u201d Right click the pre-configured project folder LTRDCN-1572 and select New File Name the new file vlan_provision.yml and hit enter. This will create the new file: Also, on the lower bar of the ATOM, verify that file grammar of YAML is selected instead of default \" Plain Text \". If YAML is not selected, then you should choose it from the listed options. Step 3: Define variables, tasks for playbook In this step, we are going to define the scope, variable for playbook and tasks In the opened window for \u2018vlan_provision.yml\u2019 file, enter the below content. NOTE: YAML is space sensitive. Hence be careful with the spaces in below section. You may copy and paste the full content (start to finish) from below to make sure that spaces are copied properly. --- #Task 2: Simple playbook assign VLAN to server facing port - hosts: leaf:jinja2_leaf vars: nxos_provider: username: \"{{ user }}\" password: \"{{ pwd }}\" transport: nxapi host: \"{{ inventory_hostname }}\" Further note: \u201c hosts: \u201d defines the scope of this playbook applies to all switches in group \u2018leaf\u2019 and \u2018jinja2_leaf\u2019 (within the \"hosts\" file created in pervious task). Note that you can review the IP addresses of the three (2) \u201cleaf\u201d and one (1) \u201cjinja2_leaf\u201d in the \u201c hosts \u201d file (configured in previous steps). The IP addresses are: jinja2_leaf: 198.18.4.104 leaf: 198.18.4.101 leaf: 198.18.4.103 \u201c vars \u201d defines one variable \u201c nxos_provider \u201d that will be used in this playbook. \u201c nxos_provider \u201d is variable that includes all value that will be used for connection and autnentication. This variable will be referenced in playbook via \u201c provider \u201d argument (that will be added in next step #4). Step 4: VLAN tasks in playbook Continue to add below tasks on the same playbook file: NOTE: YAML is space sensitive. Hence be careful with the spaces in below section. You may copy and paste the full content (start to finish) from below to make sure that spaces are copied properly. tasks: - name: provision VLAN nxos_config: lines: \"vlan {{item}}\" provider: \"{{nxos_provider}}\" with_items: - 140 - 141 tags: add vlans Note: Multiple plays can be defined in one playbook under \u201c tasks \u201d, each starts with \u201c-\u201c . This kind of play creates multiple VLANs using nxos_config module. The \u201clines\u201d option is used to pass only one configuration command. This commands must be the exact same commands as found in the device running-config. Though one or multiple configuration commands can be configured under the \u201clines\u201d option. For multiple, ordered set of commands can be configured under this \u201cline\u201d section. At the end of this play, we use \u201c tags \u201d to name the play \u201c add vlans \u201d. This is useful to run a specific part of the configuration without running whole playbook. Below screenshot shows how playbook will look: NOTE: Formatting is extremely important when working with Ansible. Ansible playbook would return errors if the spaces are not properly aligned or formatting is not correct Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. NOTE: Once the Save button is pressed, then at the lower part of ATOM app, you will see message about connecting to Ansibe host (198.18.134.150) and saving the vlan_provision.yml file Step 5: Execute playbook After creating the playbook, it is now time to execute the playbook. * Before executing the playbook, we will verify the leaf switch if it has any vlan configuration present on it. * Login to leaf-3 switch using the Mputty client, or any leaf switch in previous playbook (remember hosts: variable in the file), and execute show vlan brief command. * This command will show the vlans that currently exist on the leaf switch. As you note from below screenshot, only the default VLAN (vlan number 1) is configured: Now, go to mputty, login or launch a new ssh into Ansible node (198.18.134.150) Use command ansible-playbook vlan_provision.yml --tags \"add vlans\" under folder \"LTRDCN-1572\" as shown below: [root@rhel7-tools LTRDCN-1572]# ansible-playbook vlan_provision.yml --tags \"add vlans\" Note: If the playbook fails first time, re-run the playbook again. Make sure to save all the changes in the playbook first before executing the playbook in Ansible. After playbook is run successfully, login into leaf 3 again and check if vlan 140 and vlan 141 appears. There would also be a log message on the screen indicating a configuration change was pushed to the device: Step 6: Server port VLAN tasks in playbook We have just tested our first playbook with basic configuration (i.e, by adding 2 VLANS), now we are going to add more tasks in our existing playbook \u201c vlan_provision.yml \u201d in this step: we will add new plays in the playbook to assign VLANs to server facing port. This time, we will configure VLAN towards the server facing ports Go back to ATOM and add the following plays to the existing playbook NOTE: YAML is space sensitive. Hence be careful with the spaces in below section. You may copy and paste the full content (start to finish) from below to make sure that spaces are copied properly. - name: configure server facing port to L2 nxos_interface: interface: eth1/3 mode: layer2 provider: \"{{nxos_provider}}\" - name: configure VLAN for server port when: (\"101\" in inventory_hostname) or (\"103\" in inventory_hostname) nxos_switchport: interface: eth1/3 mode: access access_vlan: 140 provider: \"{{nxos_provider}}\" - name: configure VLAN for server port when: (\"102\" in inventory_hostname) or (\"104\" in inventory_hostname) nxos_switchport: interface: eth1/3 mode: access access_vlan: 141 provider: \"{{nxos_provider}}\" Click File and Save on ATOM. This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. Note: In this new play, we used nxos module \u201c nxos_interface \u201d and \u201c nxos_switchport \u201d. \u201c nxos_interface \u201d provides the capability to manage the physical attributes of an interface In this example, it is used to configure \u201clayer 2\u201d on interface Ethernet 1/3 \u201c nxos_switchport \u201d provides the capability to manage the Layer 2 switchport attributes In this example, it is used to configuration it is used to configure mode access on Ethernet ports 1/3 We used \u201c when \u201d argument to provide little logic of the play. In our example, the playbook assign VLAN 140 on leaf1 and leaf3 switches; assign VLAN 141 on leaf4 switch. Step 7: Execute playbook Now, we are going to execute this playbook: Before excuting the ansible playbook, you may log into switch (leaf1, leaf3 or leaf4) using MTPutty client, and check the existing configuration by executing the below command: show run interface ethernet1/3 On MTPuttY, log back into (or launch a new ssh) into \u201cAnsible\u201d node Execute command ansible-playbook vlan_provision.yml in the \u201cLTRDCN-1572\u201d directory as shown below: [root@rhel7-tools LTRDCN-1572]# ansible-playbook vlan_provision.yml Below screeenshot shows the execution of above command: After we push the configuration, login to the leaf-3 switch, and check if the server facing port has the access vlan configured with the below command: show run interface ethernet1/3 The output of above command on leaf-3 or leaf-1 is shown in below screenshot: The output of above command on leaf-4 is shown in below screenshot: Congratulation! You have created your first ansible playbook, automatically provisioned new VLANs and assigned port to new created VLANs using Ansible. Next we are going to create VXLAN Fabric using Ansible.","title":"Task 2 - First Simple Ansible Playbook"},{"location":"task2-first-ansible/#step-1-using-atom-text-editor","text":"Open \u201cAtom\u201d text editor by double click the icon on desktop. Atom is the recommended text editor for this lab: After opening ATOM, Click No, Never to the \u201cRegister as default atom:// URI handler\u201d message as show below: As an optional step: on ATOM you may close the Welcome, Welcome Guide, Telemetry Consent tabs.","title":"Step 1: Using \"Atom\" text Editor"},{"location":"task2-first-ansible/#step-2-atom-folder","text":"After opening ATOM, there should be a folder in the left pane named \u201cLTRDCN-1572.\u201d Right click the pre-configured project folder LTRDCN-1572 and select New File Name the new file vlan_provision.yml and hit enter. This will create the new file: Also, on the lower bar of the ATOM, verify that file grammar of YAML is selected instead of default \" Plain Text \". If YAML is not selected, then you should choose it from the listed options.","title":"Step 2: Atom folder"},{"location":"task2-first-ansible/#step-3-define-variables-tasks-for-playbook","text":"In this step, we are going to define the scope, variable for playbook and tasks In the opened window for \u2018vlan_provision.yml\u2019 file, enter the below content. NOTE: YAML is space sensitive. Hence be careful with the spaces in below section. You may copy and paste the full content (start to finish) from below to make sure that spaces are copied properly. --- #Task 2: Simple playbook assign VLAN to server facing port - hosts: leaf:jinja2_leaf vars: nxos_provider: username: \"{{ user }}\" password: \"{{ pwd }}\" transport: nxapi host: \"{{ inventory_hostname }}\" Further note: \u201c hosts: \u201d defines the scope of this playbook applies to all switches in group \u2018leaf\u2019 and \u2018jinja2_leaf\u2019 (within the \"hosts\" file created in pervious task). Note that you can review the IP addresses of the three (2) \u201cleaf\u201d and one (1) \u201cjinja2_leaf\u201d in the \u201c hosts \u201d file (configured in previous steps). The IP addresses are: jinja2_leaf: 198.18.4.104 leaf: 198.18.4.101 leaf: 198.18.4.103 \u201c vars \u201d defines one variable \u201c nxos_provider \u201d that will be used in this playbook. \u201c nxos_provider \u201d is variable that includes all value that will be used for connection and autnentication. This variable will be referenced in playbook via \u201c provider \u201d argument (that will be added in next step #4).","title":"Step 3: Define variables, tasks for playbook"},{"location":"task2-first-ansible/#step-4-vlan-tasks-in-playbook","text":"Continue to add below tasks on the same playbook file: NOTE: YAML is space sensitive. Hence be careful with the spaces in below section. You may copy and paste the full content (start to finish) from below to make sure that spaces are copied properly. tasks: - name: provision VLAN nxos_config: lines: \"vlan {{item}}\" provider: \"{{nxos_provider}}\" with_items: - 140 - 141 tags: add vlans Note: Multiple plays can be defined in one playbook under \u201c tasks \u201d, each starts with \u201c-\u201c . This kind of play creates multiple VLANs using nxos_config module. The \u201clines\u201d option is used to pass only one configuration command. This commands must be the exact same commands as found in the device running-config. Though one or multiple configuration commands can be configured under the \u201clines\u201d option. For multiple, ordered set of commands can be configured under this \u201cline\u201d section. At the end of this play, we use \u201c tags \u201d to name the play \u201c add vlans \u201d. This is useful to run a specific part of the configuration without running whole playbook. Below screenshot shows how playbook will look: NOTE: Formatting is extremely important when working with Ansible. Ansible playbook would return errors if the spaces are not properly aligned or formatting is not correct Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. NOTE: Once the Save button is pressed, then at the lower part of ATOM app, you will see message about connecting to Ansibe host (198.18.134.150) and saving the vlan_provision.yml file","title":"Step 4: VLAN tasks in playbook"},{"location":"task2-first-ansible/#step-5-execute-playbook","text":"After creating the playbook, it is now time to execute the playbook. * Before executing the playbook, we will verify the leaf switch if it has any vlan configuration present on it. * Login to leaf-3 switch using the Mputty client, or any leaf switch in previous playbook (remember hosts: variable in the file), and execute show vlan brief command. * This command will show the vlans that currently exist on the leaf switch. As you note from below screenshot, only the default VLAN (vlan number 1) is configured: Now, go to mputty, login or launch a new ssh into Ansible node (198.18.134.150) Use command ansible-playbook vlan_provision.yml --tags \"add vlans\" under folder \"LTRDCN-1572\" as shown below: [root@rhel7-tools LTRDCN-1572]# ansible-playbook vlan_provision.yml --tags \"add vlans\" Note: If the playbook fails first time, re-run the playbook again. Make sure to save all the changes in the playbook first before executing the playbook in Ansible. After playbook is run successfully, login into leaf 3 again and check if vlan 140 and vlan 141 appears. There would also be a log message on the screen indicating a configuration change was pushed to the device:","title":"Step 5: Execute playbook"},{"location":"task2-first-ansible/#step-6-server-port-vlan-tasks-in-playbook","text":"We have just tested our first playbook with basic configuration (i.e, by adding 2 VLANS), now we are going to add more tasks in our existing playbook \u201c vlan_provision.yml \u201d in this step: we will add new plays in the playbook to assign VLANs to server facing port. This time, we will configure VLAN towards the server facing ports Go back to ATOM and add the following plays to the existing playbook NOTE: YAML is space sensitive. Hence be careful with the spaces in below section. You may copy and paste the full content (start to finish) from below to make sure that spaces are copied properly. - name: configure server facing port to L2 nxos_interface: interface: eth1/3 mode: layer2 provider: \"{{nxos_provider}}\" - name: configure VLAN for server port when: (\"101\" in inventory_hostname) or (\"103\" in inventory_hostname) nxos_switchport: interface: eth1/3 mode: access access_vlan: 140 provider: \"{{nxos_provider}}\" - name: configure VLAN for server port when: (\"102\" in inventory_hostname) or (\"104\" in inventory_hostname) nxos_switchport: interface: eth1/3 mode: access access_vlan: 141 provider: \"{{nxos_provider}}\" Click File and Save on ATOM. This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. Note: In this new play, we used nxos module \u201c nxos_interface \u201d and \u201c nxos_switchport \u201d. \u201c nxos_interface \u201d provides the capability to manage the physical attributes of an interface In this example, it is used to configure \u201clayer 2\u201d on interface Ethernet 1/3 \u201c nxos_switchport \u201d provides the capability to manage the Layer 2 switchport attributes In this example, it is used to configuration it is used to configure mode access on Ethernet ports 1/3 We used \u201c when \u201d argument to provide little logic of the play. In our example, the playbook assign VLAN 140 on leaf1 and leaf3 switches; assign VLAN 141 on leaf4 switch.","title":"Step 6: Server port VLAN tasks in playbook"},{"location":"task2-first-ansible/#step-7-execute-playbook","text":"Now, we are going to execute this playbook: Before excuting the ansible playbook, you may log into switch (leaf1, leaf3 or leaf4) using MTPutty client, and check the existing configuration by executing the below command: show run interface ethernet1/3 On MTPuttY, log back into (or launch a new ssh) into \u201cAnsible\u201d node Execute command ansible-playbook vlan_provision.yml in the \u201cLTRDCN-1572\u201d directory as shown below: [root@rhel7-tools LTRDCN-1572]# ansible-playbook vlan_provision.yml Below screeenshot shows the execution of above command: After we push the configuration, login to the leaf-3 switch, and check if the server facing port has the access vlan configured with the below command: show run interface ethernet1/3 The output of above command on leaf-3 or leaf-1 is shown in below screenshot: The output of above command on leaf-4 is shown in below screenshot: Congratulation! You have created your first ansible playbook, automatically provisioned new VLANs and assigned port to new created VLANs using Ansible. Next we are going to create VXLAN Fabric using Ansible.","title":"Step 7: Execute playbook"},{"location":"task3-vxlan-jinja2/","text":"In this task, we are going to install Jinja2 - that provides templating option. In this section, we use Jinja2 to create template for spine and leaf and configure VXLAN fabric using this Jinja2 templates. Jinja2 template looks just like the NXOS configurations. We abstract the variables out of the configuration and use simple for loop to feed variables into the template. Step 1: Install jinja2 On the Ansible node, install jinja2 using pip install jinja2 command. If it is already installed, we will get the message \u201crequirement is satisfied\u201d: [root@rhel7-tools ~]# pip install jinja2 Below screenshot shows the output of above command: Step 2: Playbook for jinja2 Spine In this section, we will use Jina2 template and Ansible to provision the VXLAN Fabric. Switch to \u201cAtom\u201d, right click on the folder LTRDCN-1572 and Click New File to create a new playbook named jinja2_fabric.yml . Name the new file jinja2_fabric.yml and hit enter as shown below. It will create this new file: Also, on the lower bar of the ATOM, verify that file grammar of YAML is selected instead of default \" Plain Text \". If YAML is not selected, then you should choose it from the listed options. Now enter below data in this playbook: --- - hosts: jinja2_spine connection: local roles: - jinja2_spine The contents of the jinja2_fabric.yml file should look like Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package On the MTputty, go back to the ssh session to Ansible Server node (198.18.134.150). On Ansible server node (198.18.134.150), verify that the (below highlighted) 2 groups exists of jinja2_spine and jinja2_leaf in the inventory file named \" hosts \" (under LTRDCN-1572 directory) exists by issuing below commands: [root@rhel7-tools ~]# cd /root/LTRDCN-1572 [root@rhel7-tools ~]# more hosts Below screenshot confirms the ouptut and appropriate IP addresses of above command: Step 3: Create new roles and vars In this section, we will create two new roles for provisioning Fabric with Jina2 template. On the MTputty, go back to Ansible Server node (198.18.134.150), switch to \u2018 roles \u2019 directory; create \u2018jinja2_spine\u2019 and \u2018jinja2_leaf\u2019 roles using ansible-galaxy using below commands: [root@rhel7-tools ~]# cd ~/LTRDCN-1572/ [root@rhel7-tools LTRDCN-1572]# cd roles/ [root@rhel7-tools roles]# ansible-galaxy init jinja2_spine&&ansible-galaxy init jinja2_leaf Below screenshot shows the output of above command: Note: \u2018 ansible-galaxy \u2019 will initialize the role structure and create necessary folders with default name like \u2018tasks\u2019, \u2018template\u2019, \u2018vars\u2019 etc. change directory path to LTRDCN-1572/roles/jinja2_spine and check the content of local directory ( ls ) as per below commands: [root@rhel7-tools]# cd ~/LTRDCN-1572/roles/jinja2_spine/ [root@rhel7-tools jinja2_spine]# ls Below screenshot shows the output of above file. Note that various directories including tasks, templates, vars exists. We will use these in later steps. Next: Create empty jinja2 template files for spine and leaf under templates folder for each role by running below commands: [root@rhel7-tools roles]# cd ~/LTRDCN-1572/roles [root@rhel7-tools roles]# touch jinja2_spine/templates/spine.j2 [root@rhel7-tools roles]# touch jinja2_leaf/templates/leaf.j2 Switch to \u201c Atom \u201d and sync the new created folders between Ansible node and Remote desktop by pressing Right Click on the folder LTRDDCN-1572 , then open Remote Sync select Download Folder as shown below: Once the download is done you should see the new folder & files ( roles ) appear on ATOM as well. Step 4: Create variable file for \u201cjina2_spine\u201d role \u201c ansible-galaxy \u201d automatically creates empty \u201c main.yml \u201d file under \u201c vars \u201d folder. We can use \u201c Atom \u201d to edit the main.yml file to include the following variables that will be used in jinja2 template. Switch to ATOM , then open up the project folder LTRDCN-1572 from the left pane and open main.yml file under \u201c roles/jinja2_spine/vars/ \u201d as shown below: use \u201c Atom \u201d to edit the \u201c main.yml \u201d file to include the following variables that will be used in jinja2 template. You can copy and paste all of the below content into main.yml file. Note: as per steps in previous tasks, be careful with YAML content since its space sensitive. --- # vars file for jinja2_spine nxos_provider: username: \"{{ user }}\" password: \"{{ pwd }}\" timeout: 100 host: \"{{ inventory_hostname }}\" asn: 65000 bgp_neighbors: - remote_as: 65000 neighbor: 192.168.0.8 update_source: Loopback0 - remote_as: 65000 neighbor: 192.168.0.10 update_source: Loopback0 - remote_as: 65000 neighbor: 192.168.0.11 update_source: Loopback0 L3_interfaces: - interface: Ethernet 1/1 - interface: Ethernet 1/2 - interface: Ethernet 1/3 - interface: Ethernet 1/4 - interface: loopback 0 - interface: loopback 1 s1_loopback: 192.168.0.6 s2_loopback: 192.168.0.7 Contents of the \u2018 main.yml \u2019 file should look like below: Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. Step 5: Create Jinja2 template for spine role On ATOM open up the project folder LTRDCN-1572 from the left pane and open spine.j2 file under \u201c roles/jinja2_spine/templates \u201d as shown below: NOTE: If the file does not appear on the ATOM, then go ahead and execute below 4 steps to get it sync. If the spine.j2 file appears in above folder then you can skip below 4 steps : On Ansible server (198.18.134.150) using your SSH session, change Directory to folder LTRDCN-1572 using below command: [root@rhel7-tools ~]# cd ~/LTRDCN-1572 further, change Directory (cd) to folder roles/jinja2_spine/templates using below command: [root@rhel7-tools LTRDCN-1572]# cd roles/jinja2_spine/templates Type touch spine.j2 as shown below: [root@rhel7-tools templates]# touch spine.j2 After entering the command, go back to ATOM, right click on folder LTRDCN-1572 , scroll to choose option Remote Sync option and choose Download Folder as shown below: Now that the file/folder appears properly on ATOM, go ahead and proceed with further steps: To reduce the typo, you can download jina2 template from the box folder spine.j2. Below is the link to this folder: http://cs.co/LTRDCN-1572 The file would be under LTRDCN-1572/roles/jinja2_spine/templates/spine.j2 (link below) as shown in below screenshot: https://cisco.app.box.com/folder/44981931157 After the file is downloaded to the PC (on your RDP session), go to the downloads folder. Move (or copy) the file from the downloads folder and paste the file in the projects folder TFTP_Data (\\\\AD1) (X:) --> LTRDC-1572 --> roles --> jinja2_spine --> templates as shown below: After moving/copying this file to above folder location, open this file on ATOM. You do this by going to File then Open File\u2026 on ATOM, and browse to this spine.j2 file that was just saved in X:\\LTRDCN-1572\\roles\\jinja2_spine\\templates as shown below: After opening \u201c spine.j2 \u201d file from ATOM (as shown in below screenshot confirming the updated content), go to File \u2013-> Save to push template file to Ansible node: You can verify that updated file content is on Ansible server (198.18.134.150) using your SSH session by issuing below commands: [root@rhel7-tools templates]# cd /root/LTRDCN-1572/roles/jinja2_spine/templates [root@rhel7-tools templates]# more spine.j2 Partial output of above command is shown in below screenshot confirming : Step 6: Create playbook for jinja2_spine role The playbook for jinja2_spine roles has two tasks. First task uses ansible template module to generate configuration file based on jinja2 template created in last step. The configuration file is saved in \u201c file \u201d folder. Second task is push the configuration to switch. \u201c ansible-galaxy \u201d automatically creates empty \u201c main.yml \u201d file under \u201c tasks \u201d folder. We are going to use \u201c Atom \u201d to edit the main.yml file. On ATOM, open up the project folder LTRDCN-1572 and edit main.yml file under roles/jinja2_spine/tasks/ to include following: --- # tasks file for jinja2_spine - name: Generate Spine Config template: src=spine.j2 dest=roles/jinja2_spine/files/{{inventory_hostname}}.cfg - name: Push Spine Config nxos_config: src: roles/jinja2_spine/files/{{inventory_hostname}}.cfg match: none provider: \"{{ nxos_provider }}\" Contents of the \u2018 main.yml \u2019 file should look like below: Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. NOTE: In the above YAML file, ansible module named \u201c nxos_config \u201d is used. This module performs below activities: It uses source path of the file (\u201c src \u201d) that contains the configuration or configuration template to load into spine Since \u201c match \u201d option is set to none (yes), hence the module will not attempt to compare the source configuration with the running configuration on the remote device. Step 7: Run Jinja2_fabric playbook In this section you will run the playbook created in step 2 (in this task 3), this will generate configuration file for Spine-2 switche from the template. The playbook will also push the configuration file to Spine-2 switches. Run the ansible playbook by going to folder LTRDC-1572 and executing the below commands: [root@rhel7-tools roles]# cd ~/LTRDCN-1572/ [root@rhel7-tools LTRDCN-1572]# ansible-playbook jinja2_fabric.yml Note: It will take few minutes to push configuration Below screenshot shows the execution of above playbook: To verify the execution of this playbook, you can: Login to Spine-2 switch ( on MTputty ) to verify configuration has been pushed by double clicking the spine-2 icon in the left pane on MTputty. If prompted then login with credentials admin/C1sco12345 Execute show run bgp command on the switch to confirm the configurations have been provisioned (as shown below): Step 8: Modify playbook for Leaf In this section, we will use Jina2 template and Ansible to provision the VXLAN Fabric on leaf-4. We are going to add jinja2_leaf this time to the already created playbook in step 2. Switch to \u201c Atom \u201d, click on the folder LTRDDCN-1572 , edit the existing playbook jinja2_fabric.yml file for a role for leaf (named jinja2_leaf). Add the below content at the end of existing file (i.e., add the below content to existing content in this file (do not overwrite existing content): - hosts: jinja2_leaf connection: local roles: - jinja2_leaf Below screenshot shows the contents of jinja2_fabric.yml file in Atom after adding the above configs: Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. Step 9: Variable file for jinja2_leaf role On ATOM, open up the project folder LTRDCN-1572 and edit main.yml file under roles/jinja2_leaf/vars/ to include following: --- # vars file for jinja2_leaf nxos_provider: username: \"{{ user }}\" password: \"{{ pwd }}\" timeout: 100 host: \"{{ inventory_hostname }}\" asn: 65000 bgp_neighbors: - remote_as: 65000 neighbor: 192.168.0.6 update_source: Loopback0 - remote_as: 65000 neighbor: 192.168.0.7 update_source: Loopback0 rp_address: 192.168.0.100 L3_interfaces: - interface: Ethernet 1/1 - interface: Ethernet 1/2 - interface: loopback 0 - interface: loopback 1 L2VNI: - vlan_id: 140 vni: 50140 ip_add: 172.21.140.1 mask: 24 vlan_name: L2-VNI-140-Tenant1 mcast: 239.0.0.140 - vlan_id: 141 vni: 50141 ip_add: 172.21.141.1 mask: 24 vlan_name: L2-VNI-141-Tenant1 mcast: 239.0.0.141 L3VNI: - vlan_id: 999 vlan_name: L3-VNI-999-Tenant1 vni: 50999 Below screenshot shows the contents of jinja2_leaf\\vars\\main.yml file in Atom: Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. Step 10: Jinja2 template for leaf role On ATOM, open up the project folder LTRDCN-1572 and open leaf.j2 file under \u201c roles/jinja2_leaf/templates/ \u201d NOTE: if the file does not appear on the ATOM, then go ahead and execute below 4 steps to get it sync. If the leaf.j2 file appears in above folder then you can skip below 4 steps : On MTputty, change Directory to folder LTRDCN-1572 on Ansible server (198.18.134.150) using below command: cd ~/LTRDC-1572 further, change Directory (cd) to folder roles/jinja2_leaf/templates using below command: cd roles/jinja2_leaf/templates Type touch leaf.j2 After entering the command, go back to ATOM, right click on folder LTRDCN-1572 , scroll to choose option Remote Sync option and choose Download Folder Now that the file/folder appears properly on ATOM, go ahead and proceed with further steps: To reduce the typo, you can download jina2 template from the box folder leaf.j2. Below is the link to this folder: To reduce the typo, you can download jina2 template from the box folder leaf.j2. Below is the link to this folder: http://cs.co/LTRDCN-1572 The file would be under LTRDCN-1572/roles/jinja2_leaf/templates/leaf.j2 (link below) as shown in below screenshot: https://cisco.app.box.com/folder/44981518497 After the file is downloaded to the PC (on your RDP session), go to the downloads folder. Move (or copy) the file from the downloads folder and paste the file in the projects folder TFTP_Data (\\\\AD1) (X:) --> LTRDC-1572 --> roles --> jinja2_leaf --> templates as shown below: On ATOM, go to File then Open File\u2026 and browse to this leaf.j2 that was just saved in X:\\LTRDCN-1572\\roles\\jinja2_leaf\\templates as shown below screenshots: After opening \u201c leaf.j2 \u201d file from ATOM, go to File \u2013 Save to push template file to Ansible node: Step 11: Create playbook for jinja2_leaf role The playbook for jinja2_leaf roles has two tasks. First task uses ansible template module to generate configuration file based on jinja2 template created in last step. The configuration file is saved in \u201cfile\u201d folder. Second task is to push the configuration to switch. \u201c ansible-galaxy \u201d automatically creates empty \u201c main.yml \u201d file under \u201c tasks \u201d folder. We are going to use \u201c Atom \u201d to edit this main.yml file. On ATOM, open up the project folder LTRDCN-1572 and edit \u201c main.yml \u201d file under \u201c roles/jinja2_leaf/tasks/ \u201d to include following: --- # tasks file for jinja2_leaf - name: Generate Leaf Config template: src=leaf.j2 dest=roles/jinja2_leaf/files/{{inventory_hostname}}.cfg - name: Push Leaf Config nxos_config: src: roles/jinja2_leaf/files/{{inventory_hostname}}.cfg match: none provider: \"{{ nxos_provider }}\" Below screenshot shows how the contents of jinja2_leaf/taks/main.yml file looks like in Atom: Step 12: Run Jinja2_fabric playbook In this section you will run the playbook created in step 8, this will generate configuration file for Spine-2 and Leaf-4 switches. It will also push the configuration file to both switches. Before running the ansible-playbook, you may log into the leaf-4 (in MTputty SSH session) and verify that no bgp configurations exist by running show running bgp command as shown below: NOTE: It might take couple of minutes for the configuration to be pushed to via the Ansible Server. It is working in the background. On the Ansible node (in MTputty SSH session), run the command ( ansible-playbook jinja2_fabric.yml ) to execute the playbook as shown below: [root@rhel7-tools LTRDCN-1572]# ansible-playbook jinja2_fabric.yml Below screenshot shows the output of above command: After the configuration push is successful, login (on MTputty SSH session) to leaf-4 switch to verify configuration has been pushed by running below command: show running-config bgp The output of above command is shown below: Congrats: you have successfully concluded this task by using jinja2 templates with Ansible for Cisco Nexus switches","title":"Task 3 - Use of Jinja2 templates with Ansible Playbook"},{"location":"task3-vxlan-jinja2/#step-1-install-jinja2","text":"On the Ansible node, install jinja2 using pip install jinja2 command. If it is already installed, we will get the message \u201crequirement is satisfied\u201d: [root@rhel7-tools ~]# pip install jinja2 Below screenshot shows the output of above command:","title":"Step 1: Install jinja2"},{"location":"task3-vxlan-jinja2/#step-2-playbook-for-jinja2-spine","text":"In this section, we will use Jina2 template and Ansible to provision the VXLAN Fabric. Switch to \u201cAtom\u201d, right click on the folder LTRDCN-1572 and Click New File to create a new playbook named jinja2_fabric.yml . Name the new file jinja2_fabric.yml and hit enter as shown below. It will create this new file: Also, on the lower bar of the ATOM, verify that file grammar of YAML is selected instead of default \" Plain Text \". If YAML is not selected, then you should choose it from the listed options. Now enter below data in this playbook: --- - hosts: jinja2_spine connection: local roles: - jinja2_spine The contents of the jinja2_fabric.yml file should look like Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package On the MTputty, go back to the ssh session to Ansible Server node (198.18.134.150). On Ansible server node (198.18.134.150), verify that the (below highlighted) 2 groups exists of jinja2_spine and jinja2_leaf in the inventory file named \" hosts \" (under LTRDCN-1572 directory) exists by issuing below commands: [root@rhel7-tools ~]# cd /root/LTRDCN-1572 [root@rhel7-tools ~]# more hosts Below screenshot confirms the ouptut and appropriate IP addresses of above command:","title":"Step 2: Playbook for jinja2 Spine"},{"location":"task3-vxlan-jinja2/#step-3-create-new-roles-and-vars","text":"In this section, we will create two new roles for provisioning Fabric with Jina2 template. On the MTputty, go back to Ansible Server node (198.18.134.150), switch to \u2018 roles \u2019 directory; create \u2018jinja2_spine\u2019 and \u2018jinja2_leaf\u2019 roles using ansible-galaxy using below commands: [root@rhel7-tools ~]# cd ~/LTRDCN-1572/ [root@rhel7-tools LTRDCN-1572]# cd roles/ [root@rhel7-tools roles]# ansible-galaxy init jinja2_spine&&ansible-galaxy init jinja2_leaf Below screenshot shows the output of above command: Note: \u2018 ansible-galaxy \u2019 will initialize the role structure and create necessary folders with default name like \u2018tasks\u2019, \u2018template\u2019, \u2018vars\u2019 etc. change directory path to LTRDCN-1572/roles/jinja2_spine and check the content of local directory ( ls ) as per below commands: [root@rhel7-tools]# cd ~/LTRDCN-1572/roles/jinja2_spine/ [root@rhel7-tools jinja2_spine]# ls Below screenshot shows the output of above file. Note that various directories including tasks, templates, vars exists. We will use these in later steps. Next: Create empty jinja2 template files for spine and leaf under templates folder for each role by running below commands: [root@rhel7-tools roles]# cd ~/LTRDCN-1572/roles [root@rhel7-tools roles]# touch jinja2_spine/templates/spine.j2 [root@rhel7-tools roles]# touch jinja2_leaf/templates/leaf.j2 Switch to \u201c Atom \u201d and sync the new created folders between Ansible node and Remote desktop by pressing Right Click on the folder LTRDDCN-1572 , then open Remote Sync select Download Folder as shown below: Once the download is done you should see the new folder & files ( roles ) appear on ATOM as well.","title":"Step 3: Create new roles and vars"},{"location":"task3-vxlan-jinja2/#step-4-create-variable-file-for-jina2_spine-role","text":"\u201c ansible-galaxy \u201d automatically creates empty \u201c main.yml \u201d file under \u201c vars \u201d folder. We can use \u201c Atom \u201d to edit the main.yml file to include the following variables that will be used in jinja2 template. Switch to ATOM , then open up the project folder LTRDCN-1572 from the left pane and open main.yml file under \u201c roles/jinja2_spine/vars/ \u201d as shown below: use \u201c Atom \u201d to edit the \u201c main.yml \u201d file to include the following variables that will be used in jinja2 template. You can copy and paste all of the below content into main.yml file. Note: as per steps in previous tasks, be careful with YAML content since its space sensitive. --- # vars file for jinja2_spine nxos_provider: username: \"{{ user }}\" password: \"{{ pwd }}\" timeout: 100 host: \"{{ inventory_hostname }}\" asn: 65000 bgp_neighbors: - remote_as: 65000 neighbor: 192.168.0.8 update_source: Loopback0 - remote_as: 65000 neighbor: 192.168.0.10 update_source: Loopback0 - remote_as: 65000 neighbor: 192.168.0.11 update_source: Loopback0 L3_interfaces: - interface: Ethernet 1/1 - interface: Ethernet 1/2 - interface: Ethernet 1/3 - interface: Ethernet 1/4 - interface: loopback 0 - interface: loopback 1 s1_loopback: 192.168.0.6 s2_loopback: 192.168.0.7 Contents of the \u2018 main.yml \u2019 file should look like below: Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.","title":"Step 4: Create variable file for \u201cjina2_spine\u201d role"},{"location":"task3-vxlan-jinja2/#step-5-create-jinja2-template-for-spine-role","text":"On ATOM open up the project folder LTRDCN-1572 from the left pane and open spine.j2 file under \u201c roles/jinja2_spine/templates \u201d as shown below: NOTE: If the file does not appear on the ATOM, then go ahead and execute below 4 steps to get it sync. If the spine.j2 file appears in above folder then you can skip below 4 steps : On Ansible server (198.18.134.150) using your SSH session, change Directory to folder LTRDCN-1572 using below command: [root@rhel7-tools ~]# cd ~/LTRDCN-1572 further, change Directory (cd) to folder roles/jinja2_spine/templates using below command: [root@rhel7-tools LTRDCN-1572]# cd roles/jinja2_spine/templates Type touch spine.j2 as shown below: [root@rhel7-tools templates]# touch spine.j2 After entering the command, go back to ATOM, right click on folder LTRDCN-1572 , scroll to choose option Remote Sync option and choose Download Folder as shown below: Now that the file/folder appears properly on ATOM, go ahead and proceed with further steps: To reduce the typo, you can download jina2 template from the box folder spine.j2. Below is the link to this folder: http://cs.co/LTRDCN-1572 The file would be under LTRDCN-1572/roles/jinja2_spine/templates/spine.j2 (link below) as shown in below screenshot: https://cisco.app.box.com/folder/44981931157 After the file is downloaded to the PC (on your RDP session), go to the downloads folder. Move (or copy) the file from the downloads folder and paste the file in the projects folder TFTP_Data (\\\\AD1) (X:) --> LTRDC-1572 --> roles --> jinja2_spine --> templates as shown below: After moving/copying this file to above folder location, open this file on ATOM. You do this by going to File then Open File\u2026 on ATOM, and browse to this spine.j2 file that was just saved in X:\\LTRDCN-1572\\roles\\jinja2_spine\\templates as shown below: After opening \u201c spine.j2 \u201d file from ATOM (as shown in below screenshot confirming the updated content), go to File \u2013-> Save to push template file to Ansible node: You can verify that updated file content is on Ansible server (198.18.134.150) using your SSH session by issuing below commands: [root@rhel7-tools templates]# cd /root/LTRDCN-1572/roles/jinja2_spine/templates [root@rhel7-tools templates]# more spine.j2 Partial output of above command is shown in below screenshot confirming :","title":"Step 5: Create Jinja2 template for spine role"},{"location":"task3-vxlan-jinja2/#step-6-create-playbook-for-jinja2_spine-role","text":"The playbook for jinja2_spine roles has two tasks. First task uses ansible template module to generate configuration file based on jinja2 template created in last step. The configuration file is saved in \u201c file \u201d folder. Second task is push the configuration to switch. \u201c ansible-galaxy \u201d automatically creates empty \u201c main.yml \u201d file under \u201c tasks \u201d folder. We are going to use \u201c Atom \u201d to edit the main.yml file. On ATOM, open up the project folder LTRDCN-1572 and edit main.yml file under roles/jinja2_spine/tasks/ to include following: --- # tasks file for jinja2_spine - name: Generate Spine Config template: src=spine.j2 dest=roles/jinja2_spine/files/{{inventory_hostname}}.cfg - name: Push Spine Config nxos_config: src: roles/jinja2_spine/files/{{inventory_hostname}}.cfg match: none provider: \"{{ nxos_provider }}\" Contents of the \u2018 main.yml \u2019 file should look like below: Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. NOTE: In the above YAML file, ansible module named \u201c nxos_config \u201d is used. This module performs below activities: It uses source path of the file (\u201c src \u201d) that contains the configuration or configuration template to load into spine Since \u201c match \u201d option is set to none (yes), hence the module will not attempt to compare the source configuration with the running configuration on the remote device.","title":"Step 6: Create playbook for jinja2_spine role"},{"location":"task3-vxlan-jinja2/#step-7-run-jinja2_fabric-playbook","text":"In this section you will run the playbook created in step 2 (in this task 3), this will generate configuration file for Spine-2 switche from the template. The playbook will also push the configuration file to Spine-2 switches. Run the ansible playbook by going to folder LTRDC-1572 and executing the below commands: [root@rhel7-tools roles]# cd ~/LTRDCN-1572/ [root@rhel7-tools LTRDCN-1572]# ansible-playbook jinja2_fabric.yml Note: It will take few minutes to push configuration Below screenshot shows the execution of above playbook: To verify the execution of this playbook, you can: Login to Spine-2 switch ( on MTputty ) to verify configuration has been pushed by double clicking the spine-2 icon in the left pane on MTputty. If prompted then login with credentials admin/C1sco12345 Execute show run bgp command on the switch to confirm the configurations have been provisioned (as shown below):","title":"Step 7: Run Jinja2_fabric playbook"},{"location":"task3-vxlan-jinja2/#step-8-modify-playbook-for-leaf","text":"In this section, we will use Jina2 template and Ansible to provision the VXLAN Fabric on leaf-4. We are going to add jinja2_leaf this time to the already created playbook in step 2. Switch to \u201c Atom \u201d, click on the folder LTRDDCN-1572 , edit the existing playbook jinja2_fabric.yml file for a role for leaf (named jinja2_leaf). Add the below content at the end of existing file (i.e., add the below content to existing content in this file (do not overwrite existing content): - hosts: jinja2_leaf connection: local roles: - jinja2_leaf Below screenshot shows the contents of jinja2_fabric.yml file in Atom after adding the above configs: Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.","title":"Step 8: Modify playbook for Leaf"},{"location":"task3-vxlan-jinja2/#step-9-variable-file-for-jinja2_leaf-role","text":"On ATOM, open up the project folder LTRDCN-1572 and edit main.yml file under roles/jinja2_leaf/vars/ to include following: --- # vars file for jinja2_leaf nxos_provider: username: \"{{ user }}\" password: \"{{ pwd }}\" timeout: 100 host: \"{{ inventory_hostname }}\" asn: 65000 bgp_neighbors: - remote_as: 65000 neighbor: 192.168.0.6 update_source: Loopback0 - remote_as: 65000 neighbor: 192.168.0.7 update_source: Loopback0 rp_address: 192.168.0.100 L3_interfaces: - interface: Ethernet 1/1 - interface: Ethernet 1/2 - interface: loopback 0 - interface: loopback 1 L2VNI: - vlan_id: 140 vni: 50140 ip_add: 172.21.140.1 mask: 24 vlan_name: L2-VNI-140-Tenant1 mcast: 239.0.0.140 - vlan_id: 141 vni: 50141 ip_add: 172.21.141.1 mask: 24 vlan_name: L2-VNI-141-Tenant1 mcast: 239.0.0.141 L3VNI: - vlan_id: 999 vlan_name: L3-VNI-999-Tenant1 vni: 50999 Below screenshot shows the contents of jinja2_leaf\\vars\\main.yml file in Atom: Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.","title":"Step 9: Variable file for jinja2_leaf role"},{"location":"task3-vxlan-jinja2/#step-10-jinja2-template-for-leaf-role","text":"On ATOM, open up the project folder LTRDCN-1572 and open leaf.j2 file under \u201c roles/jinja2_leaf/templates/ \u201d NOTE: if the file does not appear on the ATOM, then go ahead and execute below 4 steps to get it sync. If the leaf.j2 file appears in above folder then you can skip below 4 steps : On MTputty, change Directory to folder LTRDCN-1572 on Ansible server (198.18.134.150) using below command: cd ~/LTRDC-1572 further, change Directory (cd) to folder roles/jinja2_leaf/templates using below command: cd roles/jinja2_leaf/templates Type touch leaf.j2 After entering the command, go back to ATOM, right click on folder LTRDCN-1572 , scroll to choose option Remote Sync option and choose Download Folder Now that the file/folder appears properly on ATOM, go ahead and proceed with further steps: To reduce the typo, you can download jina2 template from the box folder leaf.j2. Below is the link to this folder: To reduce the typo, you can download jina2 template from the box folder leaf.j2. Below is the link to this folder: http://cs.co/LTRDCN-1572 The file would be under LTRDCN-1572/roles/jinja2_leaf/templates/leaf.j2 (link below) as shown in below screenshot: https://cisco.app.box.com/folder/44981518497 After the file is downloaded to the PC (on your RDP session), go to the downloads folder. Move (or copy) the file from the downloads folder and paste the file in the projects folder TFTP_Data (\\\\AD1) (X:) --> LTRDC-1572 --> roles --> jinja2_leaf --> templates as shown below: On ATOM, go to File then Open File\u2026 and browse to this leaf.j2 that was just saved in X:\\LTRDCN-1572\\roles\\jinja2_leaf\\templates as shown below screenshots: After opening \u201c leaf.j2 \u201d file from ATOM, go to File \u2013 Save to push template file to Ansible node:","title":"Step 10: Jinja2 template for leaf role"},{"location":"task3-vxlan-jinja2/#step-11-create-playbook-for-jinja2_leaf-role","text":"The playbook for jinja2_leaf roles has two tasks. First task uses ansible template module to generate configuration file based on jinja2 template created in last step. The configuration file is saved in \u201cfile\u201d folder. Second task is to push the configuration to switch. \u201c ansible-galaxy \u201d automatically creates empty \u201c main.yml \u201d file under \u201c tasks \u201d folder. We are going to use \u201c Atom \u201d to edit this main.yml file. On ATOM, open up the project folder LTRDCN-1572 and edit \u201c main.yml \u201d file under \u201c roles/jinja2_leaf/tasks/ \u201d to include following: --- # tasks file for jinja2_leaf - name: Generate Leaf Config template: src=leaf.j2 dest=roles/jinja2_leaf/files/{{inventory_hostname}}.cfg - name: Push Leaf Config nxos_config: src: roles/jinja2_leaf/files/{{inventory_hostname}}.cfg match: none provider: \"{{ nxos_provider }}\" Below screenshot shows how the contents of jinja2_leaf/taks/main.yml file looks like in Atom:","title":"Step 11: Create playbook for jinja2_leaf role"},{"location":"task3-vxlan-jinja2/#step-12-run-jinja2_fabric-playbook","text":"In this section you will run the playbook created in step 8, this will generate configuration file for Spine-2 and Leaf-4 switches. It will also push the configuration file to both switches. Before running the ansible-playbook, you may log into the leaf-4 (in MTputty SSH session) and verify that no bgp configurations exist by running show running bgp command as shown below: NOTE: It might take couple of minutes for the configuration to be pushed to via the Ansible Server. It is working in the background. On the Ansible node (in MTputty SSH session), run the command ( ansible-playbook jinja2_fabric.yml ) to execute the playbook as shown below: [root@rhel7-tools LTRDCN-1572]# ansible-playbook jinja2_fabric.yml Below screenshot shows the output of above command: After the configuration push is successful, login (on MTputty SSH session) to leaf-4 switch to verify configuration has been pushed by running below command: show running-config bgp The output of above command is shown below: Congrats: you have successfully concluded this task by using jinja2 templates with Ansible for Cisco Nexus switches","title":"Step 12: Run Jinja2_fabric playbook"},{"location":"task4-vxlan-nxos/","text":"In this section, you will build remaining VXLAN fabric using Ansible NXOS modules. We will configure BGP neighbors between spine and leaf switching by using Ansible NXOS modules. The following NXOS ansible modules are used: nxos_feature Manage features on Nexus switches nxos_bgp Manage BGP config nxos_bgp_neighbor Manage BGP neighbor config Nxos_bgp_af Manage BGP address-famility config Nxos_bgp_neighbor_af Manage BGP neighbor address-famility config In comparison to Jinja2, NXOS modules are more abstract from configurations. There is no need to have knowledge of NXOS CLI to use NXOS modules. You will follow the steps to configure BGP, Multicast, VXLAN and EVPN. In each step, you will use different Ansible NXOS modules to accomplish the step. After each step, you can login the switches to verify configuration changes. Step 1: Create new playbook Again we will use roles structure to make the playbook more modular. The roles included in the new playbook are \u201c spine \u201d and \u201c leaf \u201d. Switch to \u201cAtom\u201d, right click on the folder 'LTRDDCN-1572' and create a new playbook named nxos_fabric.yml . Enter this file name and hit enter In the nxos_fabric.yml enter the content as shown below: --- - hosts: spine connection: local roles: - spine - hosts: leaf connection: local roles: - leaf Below screenshot shows the actual content: Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. Step 2: Create new roles for Spine & Leaf On the MTputty, go back to Ansible Server node (198.18.134.150), switch to \u2018 roles \u2019 directory; create \u2018spine\u2019 and \u2018leaf\u2019 roles using ansible-galaxy as per below commands: [root@rhel7-tools LTRDCN-1572]# cd ~/LTRDCN-1572/roles/ [root@rhel7-tools roles]# ansible-galaxy init spine && ansible-galaxy init leaf Below screeshot shows the output of above command: \u200b Switch to \u201c Atom \u201d and sync the new created folders between Ansible node and Remote desktop by pressing Right Click * on the folder LTRDDCN-1572 , then open Remote Sync select Download Folder as shown below: Step 3: Build playbook for Spine role - tasks: \u201c ansible-galaxy \u201d automatically creates empty \u201c main.yml \u201d file under \u201c tasks \u201d folder. We are going to use \u201c Atom \u201d to edit the main.yml file. On ATOM, open up the project folder LTRDCN-1572 and edit main.yml file under roles/spine/tasks/ to include following: Note: It is recommended to write your playbook and learn from mistakes, but file is also available in the box folder if you prefer to reuse. --- # tasks file for spine #task to configure bgp neighbor to all leaf switches - name: Enable BGP nxos_feature: feature: bgp provider: \"{{nxos_provider}}\" state: enabled tags: bgp - name: Configure BGP AS nxos_bgp: asn: \"{{ asn }}\" router_id: \"{{ router_id }}\" provider: \"{{ nxos_provider }}\" state: present tags: bgp - name: Configure BGP AF nxos_bgp_af: asn: \"{{ asn }}\" afi: ipv4 safi: unicast provider: \"{{ nxos_provider }}\" tags: bgp - name: Configure iBGP neighbors nxos_bgp_neighbor: asn: \"{{ asn }}\" neighbor: \"{{ item.neighbor }}\" remote_as: \"{{ item.remote_as }}\" update_source: \"{{ item.update_source }}\" provider: \"{{ nxos_provider }}\" with_items: \"{{ bgp_neighbors }}\" tags: bgp - name: Configure iBGP neighbor AF nxos_bgp_neighbor_af: asn: \"{{ asn }}\" neighbor: \"{{ item.neighbor }}\" afi: ipv4 safi: unicast route_reflector_client: \"true\" send_community: both provider: \"{{ nxos_provider }}\" with_items: \"{{ bgp_neighbors }}\" tags: bgp Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. Note , in the above task/main.yml file multiple NXOS ansible modules have been used: \u201c nxos_feature \u201d module provides the capability to manage features in NX-OS features. It is used to enable bgp as a feature in above configurations \u201c nxos_bgp \u201d module provides the capability to manage BGP configuration in NX-OS. Here it is used to configure bgp \u201c nxos_bgp_af \u201d module provides the capability to manage BGP Address-family configuration in NX-OS. \u201c nxos_bgp_neighbor \u201d module is used to configure the BGP Neighbour in NX-OS. \u201c nxos_bgp_neighbor_af \u201d module provides the capability to manage BGP Address-family Neighbour configuration in NX-OS. Step 4: Build playbook for Spine role - vars \u201c ansible-galaxy \u201d automatically creates empty \u201c main.yml \u201d file under \u201c vars \u201d folder. We can use \u201c Atom \u201d to edit the main.yml file Switch to ATOM , then open up the project folder LTRDCN-1572 from the left pane and open main.yml file under \u201c roles/spine/vars/ \u201d and enter below content: --- # vars file for spine nxos_provider: username: \"{{ user }}\" password: \"{{ pwd }}\" transport: nxapi timeout: 30 host: \"{{ inventory_hostname }}\" asn: 65000 bgp_neighbors: - { remote_as: 65000, neighbor: 192.168.0.8, update_source: Loopback0 } - { remote_as: 65000, neighbor: 192.168.0.10, update_source: Loopback0 } - { remote_as: 65000, neighbor: 192.168.0.11, update_source: Loopback0 } Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. Step 5: Build playbook for leaf role - tasks: \u201c ansible-galaxy \u201d automatically creates empty \u201c main.yml \u201d file under \u201c tasks \u201d folder. We can use \u201c Atom \u201d to edit the main.yml file On ATOM, open up the project folder LTRDCN-1572 and edit main.yml file under roles/leaf/tasks/ to include following: --- # tasks file for leaf #task to configure bgp neighbor to all spine switches - name: Enable BGP nxos_feature: feature: bgp provider: \"{{nxos_provider}}\" state: enabled tags: bgp - name: Configure BGP AS nxos_bgp: asn: \"{{ asn }}\" router_id: \"{{ router_id }}\" provider: \"{{ nxos_provider }}\" state: present tags: bgp - name: Configure BGP AF nxos_bgp_af: asn: \"{{ asn }}\" afi: ipv4 safi: unicast provider: \"{{ nxos_provider }}\" tags: bgp - name: Configure iBGP neighbors nxos_bgp_neighbor: asn: \"{{ asn }}\" neighbor: \"{{ item.neighbor }}\" remote_as: \"{{ item.remote_as }}\" update_source: \"{{ item.update_source }}\" provider: \"{{ nxos_provider }}\" with_items: \"{{ bgp_neighbors }}\" tags: bgp - name: Configure iBGP neighbor AF nxos_bgp_neighbor_af: asn: \"{{ asn }}\" neighbor: \"{{ item.neighbor }}\" afi: ipv4 safi: unicast send_community: both provider: \"{{ nxos_provider }}\" with_items: \"{{ bgp_neighbors }}\" tags: bgp Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. Step 6: Build playbook for leaf role - vars \u201c ansible-galaxy \u201d automatically creates empty \u201c main.yml \u201d file under \u201c vars \u201d folder. We can use \u201c Atom \u201d to edit the main.yml file Switch to ATOM , then open up the project folder LTRDCN-1572 from the left pane and open main.yml file under \u201c roles/leaf/vars/ \u201d and enter below content: --- # vars file for leaf nxos_provider: username: \"{{ user }}\" password: \"{{ pwd }}\" transport: nxapi timeout: 30 host: \"{{ inventory_hostname }}\" asn: 65000 bgp_neighbors: - { remote_as: 65000, neighbor: 192.168.0.6, update_source: Loopback0 } - { remote_as: 65000, neighbor: 192.168.0.7, update_source: Loopback0 } Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. Step 7: Execute playbook On the Ansible node (in MTputty SSH session), run the command ( ansible-playbook nxos_fabric.yml --tags \"bgp\" ) to execute the playbook as shown below: [root@rhel7-tools LTRDCN-1572]# cd ~/LTRDCN-1572 [root@rhel7-tools LTRDCN-1572]# ansible-playbook nxos_fabric.yml --tags \"bgp\" Below screenshots shows the execution of above playbook: After the configuration push is successful, login (on MTputty SSH session) to leaf-1, leaf-3 or leaf-4 or spine-1 switch to verify configuration has been pushed by running below command and BGP neighbours are operational: show ip bgp summary The output of above command providing the BGP neighbours info on Spine-1 and Leaf-3 is shown below: Step 8: Configure Multicast using Ansible NXOS In this section, we will be configuring underlay multicast to support BUM traffic in the VXLAN fabric. The NXOS modules we will be using in this section are nxos_feature Manage fatures on Nexus switchs nxos_pim_interface Manage PIM interface configuration nxos_pim_rp_address Manage static RP configuration nxos_config Manage NXOS arbitrary configuration command nxos_interface_ospf Manage configuration OSPF interface instance nxos_interface Manage physical attribute of interface Note: there is no NXOS ansible module for Anycast RP. Hence, 'nxos_config' module will be used to push the Anycast RP configuration Edit playbook for spine role Use \u201cAtom\u201d to edit the \u201cmain.yml\u201d file. Open up the project folder \u201cLTRDCN-1572\u201d and open \u201cmain.yml\u201d file under \u201croles/spine/tasks/\u201d and save the below tasks at the end the file: Note: It is recommended to write your playbook and learn from mistakes, but file is also available in the box folder if you prefer to reuse. #task to enable pim and configure anycast rp for underlay multicast - name: Enable PIM nxos_feature: feature: pim provider: \"{{nxos_provider}}\" state: enabled tags: multicast - name: Configure Anycast RP interfce nxos_interface: interface: loopback1 provider: \"{{ nxos_provider }}\" tags: multicast - name: Configure IP Address on New LP1 nxos_ip_interface: interface: loopback1 version: v4 addr: \"{{ loopback1 }}\" mask: 32 provider: \"{{ nxos_provider }}\" tags: multicast - name: Configure PIM int nxos_pim_interface: interface: \"{{ item.interface }}\" sparse: true provider: \"{{ nxos_provider }}\" with_items: \"{{L3_interfaces}}\" tags: multicast - name: Enable OSPF on New LP1 nxos_interface_ospf: interface: loopback1 ospf: 1 area: 0 provider: \"{{ nxos_provider }}\" tags: multicast - name: Configure PIM RP nxos_pim_rp_address: rp_address: \"{{ loopback1 }}\" provider: \"{{ nxos_provider }}\" tags: multicast - name: Configure Anycast RP nxos_config: lines: - \"ip pim anycast-rp {{ loopback1 }} {{ s1_loopback }}\" - \"ip pim anycast-rp {{ loopback1 }} {{ s2_loopback }}\" provider: \"{{ nxos_provider }}\" tags: multicast Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. Edit variable file for Spine role Use \u201cAtom\u201d to edit the variables file for Spine i.e. \u201cmain.yml\u201d file. Open up the project folder \u201cLTRDCN-1572\u201d and add the below content at the end of \u201cmain.yml\u201d file under \u201croles/spine/vars/\u201d L3_interfaces: - { interface: Ethernet1/1 } - { interface: Ethernet1/2 } - { interface: Ethernet1/3 } - { interface: Ethernet1/4 } - { interface: loopback0 } - { interface: loopback1 } s1_loopback: 192.168.0.6 s2_loopback: 192.168.0.7 Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. Edit playbook for leaf role use \u201cAtom\u201d to edit the \u201cmain.yml\u201d file. Open up the project folder \u201cLTRDCN-1572\u201d and add below content at the end of \u201cmain.yml\u201d file under \u201croles/leaf/tasks/\u201d . On Atom, Make sure to click File->Save after entering the below data in this file so it is pushed to Ansible server: Note: It is recommended to write your playbook and learn from mistakes, but file is also available in the box folder if you prefer to reuse. #task to enable PIM for underlay multicast - name: Enable PIM nxos_feature: feature: pim provider: \"{{nxos_provider}}\" state: enabled tags: multicast - name: Configure PIM int nxos_pim_interface: interface: \"{{ item.interface }}\" sparse: true provider: \"{{ nxos_provider }}\" with_items: \"{{L3_interfaces}}\" tags: multicast - name: Configure PIM RP nxos_pim_rp_address: rp_address: \"{{ rp_address }}\" provider: \"{{ nxos_provider }}\" tags: multicast Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. Edit variable file for leaf role Use \u201cAtom\u201d to edit the main.yml file. Open up the project folder \u201cLTRDCN-1572\u201d and add below content at the end of \u201cmain.yml\u201d file under \u201croles/leaf/vars/\u201d . On Atom, Make sure to click File->Save after entering the below data in this file so it is pushed to Ansible server: rp_address: 192.168.0.100 L3_interfaces: - { interface: Ethernet1/1 } - { interface: Ethernet1/2 } - { interface: loopback0 } - { interface: loopback1 } Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. Run the playbook and verify configuration changes Execute the playbook by running ansible-playbook nxos_fabric.yml --tags \"multicast\" command on Ansible server as shown below : [root@rhel7-tools LTRDCN-1572]# ansible-playbook nxos_fabric.yml --tags \"multicast\" Below screenshots shows the output of above command: login to any leaf (leaf1, leaf3, leaf4) or Spine-1 switch (via SSH using MTPutty) to verify multicast configuration and PIM neighbors by executing command: show ip pim neighbor Below screenshot shows the output of above command ( show ip pim neighbor ) from Spine-1: This confirms Multicast has been enabled using Ansible modules Step 9: Configure VXLAN using Ansible In this section, we will be configuring VXLAN on leaf and spine switches. The NXOS modules we will be using in this section are nxos_feature Manages features on Nexus switches nxos_evpn_global Handles EVPN control plane for VXLAN nxos_vlan Manages VLAN resources and attributes nxos_vrf Manages global VRF configuration nxos_vrf_af Manages VRF address falimily nxos_overlay_global Configuration anycast gateway MAC nxos_vxlan_vtep Manages VXLAN Network Virtualization Endpoint nxos_vxlan_vtep_vni Creates Virtual Network Identifier member Edit playbook for spine role Use \u201cAtom\u201d to edit the \u201c main.yml \u201d file. Open up the project folder \u201cLTRDCN-1572\u201d and open \u201c main.yml \u201d file under \u201croles/spine/tasks/\u201d and enter the below content (you may copy & paste with correct spaces) at the end of the file: #task to configure vxlan fabric - name: Enable VXLAN Feature nxos_feature: feature: \"{{item}}\" provider: \"{{nxos_provider }}\" state: enabled with_items: - nv overlay - vn-segment-vlan-based tags: vxlan - name: Enable NV Overlay nxos_evpn_global: nv_overlay_evpn: true provider: \"{{ nxos_provider }}\" tags: vxlan Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. Edit variable file for Spine role No new variable required for Spine Edit playbook for leaf role use \u201cAtom\u201d to edit the main.yml file. Open up the project folder \u201cLTRDCN-1572\u201d and open \u201c main.yml \u201d file under \u201croles/leaf/tasks/\u201d . Add the below content in the file at the end of the file (i.e., in addition to existing content), and then make sure to click \u201cFile\u201d->\u201cSave\u201d on Atom, so that the updated file is pushed to Ansible server. #task to configure VXLAN fabric - name: Enable VXLAN Feature nxos_feature: feature: \"{{ item }}\" provider: \"{{nxos_provider}}\" state: enabled with_items: - nv overlay - vn-segment-vlan-based tags: vxlan - name: Enable NV Overlay nxos_evpn_global: nv_overlay_evpn: true provider: \"{{ nxos_provider }}\" tags: vxlan - name: Configure VLAN to VNI nxos_vlan: vlan_id: \"{{ item.vlan_id }}\" mapped_vni: \"{{ item.vni }}\" name: \"{{ item.vlan_name }}\" provider: \"{{ nxos_provider }}\" with_items: - \"{{ L2VNI }}\" - \"{{ L3VNI }}\" tags: vxlan - name: Configure Tenant VRF nxos_vrf: vrf: Tenant-1 rd: auto vni: \"{{ L3VNI[0].vni }}\" provider: \"{{ nxos_provider }}\" tags: vxlan - name: Configure VRF AF nxos_vrf_af: vrf: Tenant-1 route_target_both_auto_evpn: true afi: ipv4 provider: \"{{ nxos_provider }}\" tags: vxlan - name: Configure Anycast GW nxos_overlay_global: anycast_gateway_mac: 0000.2222.3333 provider: \"{{ nxos_provider }}\" tags: vxlan - name: Configure L2VNI nxos_interface: interface: vlan\"{{ item.vlan_id }}\" provider: \"{{ nxos_provider }}\" with_items: \"{{ L2VNI }}\" tags: vxlan - name: Configure L3VNI nxos_interface: interface: vlan\"{{ L3VNI[0].vlan_id }}\" provider: \"{{ nxos_provider }}\" tags: vxlan - name: Assign interface to Tenant VRF nxos_vrf_interface: vrf: Tenant-1 interface: \"vlan{{ item.vlan_id }}\" provider: \"{{ nxos_provider }}\" with_items: - \"{{ L2VNI }}\" - \"{{ L3VNI }}\" tags: vxlan - name: Configure SVI IP nxos_ip_interface: interface: \"vlan{{ item.vlan_id }}\" addr: \"{{ item.ip_add }}\" mask: \"{{ item.mask }}\" provider: \"{{ nxos_provider }}\" with_items: \"{{ L2VNI }}\" tags: vxlan - name: Configure L2VNI SVI nxos_interface: interface: vlan\"{{ item.vlan_id }}\" fabric_forwarding_anycast_gateway: true provider: \"{{ nxos_provider }}\" with_items: \"{{ L2VNI }}\" tags: vxlan - name: Configure L3VNI SVI nxos_interface: interface: vlan\"{{ L3VNI[0].vlan_id }}\" ip_forward: enable provider: \"{{ nxos_provider }}\" tags: vxlan - name: Configure VTEP Tunnel nxos_vxlan_vtep: interface: nve1 shutdown: \"false\" source_interface: Loopback1 host_reachability: \"true\" provider: \"{{ nxos_provider }}\" tags: vxlan - name: Configure L2VNI to VTEP nxos_vxlan_vtep_vni: interface: nve1 vni: \"{{ item.vni }}\" multicast_group: \"{{ item.mcast }}\" provider: \"{{ nxos_provider }}\" with_items: \"{{ L2VNI }}\" tags: vxlan - name: Configure L3VNI to VTEP nxos_vxlan_vtep_vni: interface: nve1 vni: \"{{ L3VNI[0].vni }}\" assoc_vrf: true provider: \"{{ nxos_provider }}\" tags: vxlan Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. Edit variable file for leaf role Use \u201cAtom\u201d to edit the main.yml file. Open up the project folder \u201cLTRDCN-1572\u201d and open \" main.yml \"\" file under \u201croles/leaf/vars/\u201d . Add the below content in the file at the end of the file (i.e., in addition to existing content), and then make sure to click \u201cFile\u201d->\u201cSave\u201d on Atom, so that the updated file is pushed to Ansible server. L2VNI: - { vlan_id: 140, vni: 50140, ip_add: 172.21.140.1, mask: 24, vlan_name: L2-VNI-140-Tenant1, mcast: 239.0.0.140 } - { vlan_id: 141, vni: 50141, ip_add: 172.21.141.1, mask: 24, vlan_name: L2-VNI-141-Tenant1, mcast: 239.0.0.141 } L3VNI: - { vlan_id: 999, vlan_name: L3-VNI-999-Tenant1, vni: 50999 } Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. Run the playbook and verify configuration changes On Ansible server (via SSH connection on MTputty), run the below command: [root@rhel7-tools LTRDCN-1572]# ansible-playbook nxos_fabric.yml --tags \"vxlan\" Below screenshot show partial output of above command: After finishing this task: login to any leaf switch (on MTPutty) to verify VXLAN configuration and the VNI by issuing the command: show nve vni Below screenshot shows the output of above command from Leaf-3: ![](pic/4-5-2.png) Step 10: Configure EVPN using Ansible In this section, we will be configuring BGP EVPN on leaf and spine switches. The NXOS modules we will be using in this section are: nxos_bgp_af Manage BGP address-famility config nxos_bgp_neighbor_af Manage BGP neighbor address-famility config nxos_evpn_vni Manage Cisco EVPN VXLAN Network Identifier Edit playbook for spine role Use \u201cAtom\u201d to edit the main.yml file. Open up the project folder \u201cLTRDCN-1572\u201d and open \u201c main.yml \u201d file under \u201croles/spine/tasks/\u201d . Add the below content in the file in addition to existing content. Then make sure to click \u201cFile\u201d->\u201cSave\u201d on Atom, so that the updated file is pushed to Ansible server. # task to configure BGP EVPN - name: Configure BGP EVPN nxos_bgp_af: asn: \"{{ asn }}\" afi: l2vpn safi: evpn provider: \"{{ nxos_provider }}\" tags: evpn - name: Configure iBGP neighbor EVPN AF nxos_bgp_neighbor_af: asn: \"{{ asn }}\" neighbor: \"{{ item.neighbor }}\" afi: l2vpn safi: evpn route_reflector_client: \"true\" send_community: both provider: \"{{ nxos_provider }}\" with_items: \"{{ bgp_neighbors }}\" tags: evpn Edit variable file for Spine role No new variables required for Spine Edit playbook for leaf role use \u201cAtom\u201d to edit the \u201c main.yml \u201d file. Open up the project folder \u201cLTRDCN-1572\u201d and open \u201c main.yml \u201d file under \u201croles/leaf/tasks/\u201d . Add the below content in the file in addition to existing content. Then make sure to click \u201cFile\u201d->\u201cSave\u201d on Atom, so that the updated file is pushed to Ansible server. #task to configure BGP EVPN - name: Configure BGP EVPN nxos_bgp_af: asn: \"{{ asn }}\" afi: l2vpn safi: evpn provider: \"{{ nxos_provider }}\" tags: evpn - name: Configure iBGP neighbor EVPN AF nxos_bgp_neighbor_af: asn: \"{{ asn }}\" neighbor: \"{{ item.neighbor }}\" afi: l2vpn safi: evpn send_community: both provider: \"{{ nxos_provider }}\" with_items: \"{{ bgp_neighbors }}\" tags: evpn - name: Configure L2VNI RD/RT nxos_evpn_vni: vni: \"{{ item.vni }}\" route_distinguisher: auto route_target_both: auto provider: \"{{ nxos_provider }}\" with_items: \"{{ L2VNI }}\" tags: evpn Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. Edit variable file for leaf role No new variables required for Leaf Run the playbook and verify configuration changes On the Ansible server (using MTPutty) run this playbook by running the command ansible-playbook nxos_fabric.yml --tags \"evpn\" as shown below: [root@rhel7-tools LTRDCN-1572]# ansible-playbook nxos_fabric.yml --tags \"evpn\" Below screenshot shows the output of above playbook: After successful execution of the playbook: login to any leaf or spine switch to verify BGP EVPN configuration and evpn neighbor by issuing command: show bgp l2vpn evpn summary Below screenshot shows the output of above command from leaf-3 switch. As expected it shows the spine-1 and spine-2 as neighbors: Step 11: Run nxos_fabric playbook Up to this point, you have run the playbook for each step separately (using tags). You could re-run the whole playbook without giving any tags, but no new changes should be maded to the switches. [root@rhel7-tools LTRDCN-1572]# ansible-playbook nxos_fabric.yml Step 12: Verify end-to-end IP connectivity Now Let\u2019s verify the VXLAN bridging and VXLAN routing from servers that are pre-configured in following VLANs and IPs Server Name Connect to switch In VLAN IP of server Server-1 Leaf-1 140 172.21.140.10 Server-3 Leaf-3 140 172.21.140.11 Server-4 Leaf-4 141 172.21.141.11 Below figure shows the topology & connectivity of servers to Leaf switches and their respective IP addresses: Switch to MTPuTTY and SSH to server-1 . If prompted enter the credentials of root/C1sco12345 Ping default gateway from server-1 by issuing command ping 172.21.140.1 as shown below: [root@server-1 ~]# ping 172.21.140.1 PING 172.21.140.1 (172.21.140.1) 56(84) bytes of data. 64 bytes from 172.21.140.1: icmp_seq=2 ttl=255 time=15.7 ms 64 bytes from 172.21.140.1: icmp_seq=3 ttl=255 time=4.11 ms Next, Ping server 3 and server 4 from server-1 (in same VLAN and inter-VLAN respectively) by using ping 172.21.140.11 and ping 172.21.141.11 commands as shown below: [[root@server-1 ~]# ping 172.21.140.11 PING 172.21.140.11 (172.21.140.11) 56(84) bytes of data. 64 bytes from 172.21.140.11: icmp_seq=1 ttl=64 time=1032 ms 64 bytes from 172.21.140.11: icmp_seq=2 ttl=64 time=35.7 ms 64 bytes from 172.21.140.11: icmp_seq=3 ttl=64 time=14.4 ms ^C --- 172.21.140.11 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2112ms rtt min/avg/max/mdev = 14.431/360.839/1032.335/474.899 ms, pipe 2 [root@server-1 ~]# ping 172.21.141.11 PING 172.21.141.11 (172.21.141.11) 56(84) bytes of data. 64 bytes from 172.21.141.11: icmp_seq=994 ttl=62 time=30.2 ms 64 bytes from 172.21.141.11: icmp_seq=995 ttl=62 time=16.1 ms 64 bytes from 172.21.141.11: icmp_seq=996 ttl=62 time=18.0 ms Congratulation! You have successfuly built VXLAN fabric using ansible + Jinja2 template and ansible + NXOS modules.","title":"Task 4 - Config switches using Ansible NXOS modules"},{"location":"task4-vxlan-nxos/#step-1-create-new-playbook","text":"Again we will use roles structure to make the playbook more modular. The roles included in the new playbook are \u201c spine \u201d and \u201c leaf \u201d. Switch to \u201cAtom\u201d, right click on the folder 'LTRDDCN-1572' and create a new playbook named nxos_fabric.yml . Enter this file name and hit enter In the nxos_fabric.yml enter the content as shown below: --- - hosts: spine connection: local roles: - spine - hosts: leaf connection: local roles: - leaf Below screenshot shows the actual content: Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.","title":"Step 1: Create new playbook"},{"location":"task4-vxlan-nxos/#step-2-create-new-roles-for-spine-leaf","text":"On the MTputty, go back to Ansible Server node (198.18.134.150), switch to \u2018 roles \u2019 directory; create \u2018spine\u2019 and \u2018leaf\u2019 roles using ansible-galaxy as per below commands: [root@rhel7-tools LTRDCN-1572]# cd ~/LTRDCN-1572/roles/ [root@rhel7-tools roles]# ansible-galaxy init spine && ansible-galaxy init leaf Below screeshot shows the output of above command: \u200b Switch to \u201c Atom \u201d and sync the new created folders between Ansible node and Remote desktop by pressing Right Click * on the folder LTRDDCN-1572 , then open Remote Sync select Download Folder as shown below:","title":"Step 2: Create new roles for Spine &amp; Leaf"},{"location":"task4-vxlan-nxos/#step-3-build-playbook-for-spine-role-tasks","text":"\u201c ansible-galaxy \u201d automatically creates empty \u201c main.yml \u201d file under \u201c tasks \u201d folder. We are going to use \u201c Atom \u201d to edit the main.yml file. On ATOM, open up the project folder LTRDCN-1572 and edit main.yml file under roles/spine/tasks/ to include following: Note: It is recommended to write your playbook and learn from mistakes, but file is also available in the box folder if you prefer to reuse. --- # tasks file for spine #task to configure bgp neighbor to all leaf switches - name: Enable BGP nxos_feature: feature: bgp provider: \"{{nxos_provider}}\" state: enabled tags: bgp - name: Configure BGP AS nxos_bgp: asn: \"{{ asn }}\" router_id: \"{{ router_id }}\" provider: \"{{ nxos_provider }}\" state: present tags: bgp - name: Configure BGP AF nxos_bgp_af: asn: \"{{ asn }}\" afi: ipv4 safi: unicast provider: \"{{ nxos_provider }}\" tags: bgp - name: Configure iBGP neighbors nxos_bgp_neighbor: asn: \"{{ asn }}\" neighbor: \"{{ item.neighbor }}\" remote_as: \"{{ item.remote_as }}\" update_source: \"{{ item.update_source }}\" provider: \"{{ nxos_provider }}\" with_items: \"{{ bgp_neighbors }}\" tags: bgp - name: Configure iBGP neighbor AF nxos_bgp_neighbor_af: asn: \"{{ asn }}\" neighbor: \"{{ item.neighbor }}\" afi: ipv4 safi: unicast route_reflector_client: \"true\" send_community: both provider: \"{{ nxos_provider }}\" with_items: \"{{ bgp_neighbors }}\" tags: bgp Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. Note , in the above task/main.yml file multiple NXOS ansible modules have been used: \u201c nxos_feature \u201d module provides the capability to manage features in NX-OS features. It is used to enable bgp as a feature in above configurations \u201c nxos_bgp \u201d module provides the capability to manage BGP configuration in NX-OS. Here it is used to configure bgp \u201c nxos_bgp_af \u201d module provides the capability to manage BGP Address-family configuration in NX-OS. \u201c nxos_bgp_neighbor \u201d module is used to configure the BGP Neighbour in NX-OS. \u201c nxos_bgp_neighbor_af \u201d module provides the capability to manage BGP Address-family Neighbour configuration in NX-OS.","title":"Step 3: Build playbook for Spine role - tasks:"},{"location":"task4-vxlan-nxos/#step-4-build-playbook-for-spine-role-vars","text":"\u201c ansible-galaxy \u201d automatically creates empty \u201c main.yml \u201d file under \u201c vars \u201d folder. We can use \u201c Atom \u201d to edit the main.yml file Switch to ATOM , then open up the project folder LTRDCN-1572 from the left pane and open main.yml file under \u201c roles/spine/vars/ \u201d and enter below content: --- # vars file for spine nxos_provider: username: \"{{ user }}\" password: \"{{ pwd }}\" transport: nxapi timeout: 30 host: \"{{ inventory_hostname }}\" asn: 65000 bgp_neighbors: - { remote_as: 65000, neighbor: 192.168.0.8, update_source: Loopback0 } - { remote_as: 65000, neighbor: 192.168.0.10, update_source: Loopback0 } - { remote_as: 65000, neighbor: 192.168.0.11, update_source: Loopback0 } Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.","title":"Step 4:  Build playbook for Spine role - vars"},{"location":"task4-vxlan-nxos/#step-5-build-playbook-for-leaf-role-tasks","text":"\u201c ansible-galaxy \u201d automatically creates empty \u201c main.yml \u201d file under \u201c tasks \u201d folder. We can use \u201c Atom \u201d to edit the main.yml file On ATOM, open up the project folder LTRDCN-1572 and edit main.yml file under roles/leaf/tasks/ to include following: --- # tasks file for leaf #task to configure bgp neighbor to all spine switches - name: Enable BGP nxos_feature: feature: bgp provider: \"{{nxos_provider}}\" state: enabled tags: bgp - name: Configure BGP AS nxos_bgp: asn: \"{{ asn }}\" router_id: \"{{ router_id }}\" provider: \"{{ nxos_provider }}\" state: present tags: bgp - name: Configure BGP AF nxos_bgp_af: asn: \"{{ asn }}\" afi: ipv4 safi: unicast provider: \"{{ nxos_provider }}\" tags: bgp - name: Configure iBGP neighbors nxos_bgp_neighbor: asn: \"{{ asn }}\" neighbor: \"{{ item.neighbor }}\" remote_as: \"{{ item.remote_as }}\" update_source: \"{{ item.update_source }}\" provider: \"{{ nxos_provider }}\" with_items: \"{{ bgp_neighbors }}\" tags: bgp - name: Configure iBGP neighbor AF nxos_bgp_neighbor_af: asn: \"{{ asn }}\" neighbor: \"{{ item.neighbor }}\" afi: ipv4 safi: unicast send_community: both provider: \"{{ nxos_provider }}\" with_items: \"{{ bgp_neighbors }}\" tags: bgp Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.","title":"Step 5:  Build playbook for leaf role - tasks:"},{"location":"task4-vxlan-nxos/#step-6-build-playbook-for-leaf-role-vars","text":"\u201c ansible-galaxy \u201d automatically creates empty \u201c main.yml \u201d file under \u201c vars \u201d folder. We can use \u201c Atom \u201d to edit the main.yml file Switch to ATOM , then open up the project folder LTRDCN-1572 from the left pane and open main.yml file under \u201c roles/leaf/vars/ \u201d and enter below content: --- # vars file for leaf nxos_provider: username: \"{{ user }}\" password: \"{{ pwd }}\" transport: nxapi timeout: 30 host: \"{{ inventory_hostname }}\" asn: 65000 bgp_neighbors: - { remote_as: 65000, neighbor: 192.168.0.6, update_source: Loopback0 } - { remote_as: 65000, neighbor: 192.168.0.7, update_source: Loopback0 } Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.","title":"Step 6: Build playbook for leaf role - vars"},{"location":"task4-vxlan-nxos/#step-7-execute-playbook","text":"On the Ansible node (in MTputty SSH session), run the command ( ansible-playbook nxos_fabric.yml --tags \"bgp\" ) to execute the playbook as shown below: [root@rhel7-tools LTRDCN-1572]# cd ~/LTRDCN-1572 [root@rhel7-tools LTRDCN-1572]# ansible-playbook nxos_fabric.yml --tags \"bgp\" Below screenshots shows the execution of above playbook: After the configuration push is successful, login (on MTputty SSH session) to leaf-1, leaf-3 or leaf-4 or spine-1 switch to verify configuration has been pushed by running below command and BGP neighbours are operational: show ip bgp summary The output of above command providing the BGP neighbours info on Spine-1 and Leaf-3 is shown below:","title":"Step 7: Execute playbook"},{"location":"task4-vxlan-nxos/#step-8-configure-multicast-using-ansible-nxos","text":"In this section, we will be configuring underlay multicast to support BUM traffic in the VXLAN fabric. The NXOS modules we will be using in this section are nxos_feature Manage fatures on Nexus switchs nxos_pim_interface Manage PIM interface configuration nxos_pim_rp_address Manage static RP configuration nxos_config Manage NXOS arbitrary configuration command nxos_interface_ospf Manage configuration OSPF interface instance nxos_interface Manage physical attribute of interface Note: there is no NXOS ansible module for Anycast RP. Hence, 'nxos_config' module will be used to push the Anycast RP configuration","title":"Step 8: Configure Multicast using Ansible NXOS"},{"location":"task4-vxlan-nxos/#edit-playbook-for-spine-role","text":"Use \u201cAtom\u201d to edit the \u201cmain.yml\u201d file. Open up the project folder \u201cLTRDCN-1572\u201d and open \u201cmain.yml\u201d file under \u201croles/spine/tasks/\u201d and save the below tasks at the end the file: Note: It is recommended to write your playbook and learn from mistakes, but file is also available in the box folder if you prefer to reuse. #task to enable pim and configure anycast rp for underlay multicast - name: Enable PIM nxos_feature: feature: pim provider: \"{{nxos_provider}}\" state: enabled tags: multicast - name: Configure Anycast RP interfce nxos_interface: interface: loopback1 provider: \"{{ nxos_provider }}\" tags: multicast - name: Configure IP Address on New LP1 nxos_ip_interface: interface: loopback1 version: v4 addr: \"{{ loopback1 }}\" mask: 32 provider: \"{{ nxos_provider }}\" tags: multicast - name: Configure PIM int nxos_pim_interface: interface: \"{{ item.interface }}\" sparse: true provider: \"{{ nxos_provider }}\" with_items: \"{{L3_interfaces}}\" tags: multicast - name: Enable OSPF on New LP1 nxos_interface_ospf: interface: loopback1 ospf: 1 area: 0 provider: \"{{ nxos_provider }}\" tags: multicast - name: Configure PIM RP nxos_pim_rp_address: rp_address: \"{{ loopback1 }}\" provider: \"{{ nxos_provider }}\" tags: multicast - name: Configure Anycast RP nxos_config: lines: - \"ip pim anycast-rp {{ loopback1 }} {{ s1_loopback }}\" - \"ip pim anycast-rp {{ loopback1 }} {{ s2_loopback }}\" provider: \"{{ nxos_provider }}\" tags: multicast Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.","title":"Edit playbook for spine role"},{"location":"task4-vxlan-nxos/#edit-variable-file-for-spine-role","text":"Use \u201cAtom\u201d to edit the variables file for Spine i.e. \u201cmain.yml\u201d file. Open up the project folder \u201cLTRDCN-1572\u201d and add the below content at the end of \u201cmain.yml\u201d file under \u201croles/spine/vars/\u201d L3_interfaces: - { interface: Ethernet1/1 } - { interface: Ethernet1/2 } - { interface: Ethernet1/3 } - { interface: Ethernet1/4 } - { interface: loopback0 } - { interface: loopback1 } s1_loopback: 192.168.0.6 s2_loopback: 192.168.0.7 Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.","title":"Edit variable file for Spine role"},{"location":"task4-vxlan-nxos/#edit-playbook-for-leaf-role","text":"use \u201cAtom\u201d to edit the \u201cmain.yml\u201d file. Open up the project folder \u201cLTRDCN-1572\u201d and add below content at the end of \u201cmain.yml\u201d file under \u201croles/leaf/tasks/\u201d . On Atom, Make sure to click File->Save after entering the below data in this file so it is pushed to Ansible server: Note: It is recommended to write your playbook and learn from mistakes, but file is also available in the box folder if you prefer to reuse. #task to enable PIM for underlay multicast - name: Enable PIM nxos_feature: feature: pim provider: \"{{nxos_provider}}\" state: enabled tags: multicast - name: Configure PIM int nxos_pim_interface: interface: \"{{ item.interface }}\" sparse: true provider: \"{{ nxos_provider }}\" with_items: \"{{L3_interfaces}}\" tags: multicast - name: Configure PIM RP nxos_pim_rp_address: rp_address: \"{{ rp_address }}\" provider: \"{{ nxos_provider }}\" tags: multicast Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.","title":"Edit playbook for leaf role"},{"location":"task4-vxlan-nxos/#edit-variable-file-for-leaf-role","text":"Use \u201cAtom\u201d to edit the main.yml file. Open up the project folder \u201cLTRDCN-1572\u201d and add below content at the end of \u201cmain.yml\u201d file under \u201croles/leaf/vars/\u201d . On Atom, Make sure to click File->Save after entering the below data in this file so it is pushed to Ansible server: rp_address: 192.168.0.100 L3_interfaces: - { interface: Ethernet1/1 } - { interface: Ethernet1/2 } - { interface: loopback0 } - { interface: loopback1 } Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.","title":"Edit variable file for leaf role"},{"location":"task4-vxlan-nxos/#run-the-playbook-and-verify-configuration-changes","text":"Execute the playbook by running ansible-playbook nxos_fabric.yml --tags \"multicast\" command on Ansible server as shown below : [root@rhel7-tools LTRDCN-1572]# ansible-playbook nxos_fabric.yml --tags \"multicast\" Below screenshots shows the output of above command: login to any leaf (leaf1, leaf3, leaf4) or Spine-1 switch (via SSH using MTPutty) to verify multicast configuration and PIM neighbors by executing command: show ip pim neighbor Below screenshot shows the output of above command ( show ip pim neighbor ) from Spine-1: This confirms Multicast has been enabled using Ansible modules","title":"Run the playbook and verify configuration changes"},{"location":"task4-vxlan-nxos/#step-9-configure-vxlan-using-ansible","text":"In this section, we will be configuring VXLAN on leaf and spine switches. The NXOS modules we will be using in this section are nxos_feature Manages features on Nexus switches nxos_evpn_global Handles EVPN control plane for VXLAN nxos_vlan Manages VLAN resources and attributes nxos_vrf Manages global VRF configuration nxos_vrf_af Manages VRF address falimily nxos_overlay_global Configuration anycast gateway MAC nxos_vxlan_vtep Manages VXLAN Network Virtualization Endpoint nxos_vxlan_vtep_vni Creates Virtual Network Identifier member","title":"Step 9: Configure VXLAN using Ansible"},{"location":"task4-vxlan-nxos/#edit-playbook-for-spine-role_1","text":"Use \u201cAtom\u201d to edit the \u201c main.yml \u201d file. Open up the project folder \u201cLTRDCN-1572\u201d and open \u201c main.yml \u201d file under \u201croles/spine/tasks/\u201d and enter the below content (you may copy & paste with correct spaces) at the end of the file: #task to configure vxlan fabric - name: Enable VXLAN Feature nxos_feature: feature: \"{{item}}\" provider: \"{{nxos_provider }}\" state: enabled with_items: - nv overlay - vn-segment-vlan-based tags: vxlan - name: Enable NV Overlay nxos_evpn_global: nv_overlay_evpn: true provider: \"{{ nxos_provider }}\" tags: vxlan Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.","title":"Edit playbook for spine role"},{"location":"task4-vxlan-nxos/#edit-variable-file-for-spine-role_1","text":"No new variable required for Spine","title":"Edit variable file for Spine role"},{"location":"task4-vxlan-nxos/#edit-playbook-for-leaf-role_1","text":"use \u201cAtom\u201d to edit the main.yml file. Open up the project folder \u201cLTRDCN-1572\u201d and open \u201c main.yml \u201d file under \u201croles/leaf/tasks/\u201d . Add the below content in the file at the end of the file (i.e., in addition to existing content), and then make sure to click \u201cFile\u201d->\u201cSave\u201d on Atom, so that the updated file is pushed to Ansible server. #task to configure VXLAN fabric - name: Enable VXLAN Feature nxos_feature: feature: \"{{ item }}\" provider: \"{{nxos_provider}}\" state: enabled with_items: - nv overlay - vn-segment-vlan-based tags: vxlan - name: Enable NV Overlay nxos_evpn_global: nv_overlay_evpn: true provider: \"{{ nxos_provider }}\" tags: vxlan - name: Configure VLAN to VNI nxos_vlan: vlan_id: \"{{ item.vlan_id }}\" mapped_vni: \"{{ item.vni }}\" name: \"{{ item.vlan_name }}\" provider: \"{{ nxos_provider }}\" with_items: - \"{{ L2VNI }}\" - \"{{ L3VNI }}\" tags: vxlan - name: Configure Tenant VRF nxos_vrf: vrf: Tenant-1 rd: auto vni: \"{{ L3VNI[0].vni }}\" provider: \"{{ nxos_provider }}\" tags: vxlan - name: Configure VRF AF nxos_vrf_af: vrf: Tenant-1 route_target_both_auto_evpn: true afi: ipv4 provider: \"{{ nxos_provider }}\" tags: vxlan - name: Configure Anycast GW nxos_overlay_global: anycast_gateway_mac: 0000.2222.3333 provider: \"{{ nxos_provider }}\" tags: vxlan - name: Configure L2VNI nxos_interface: interface: vlan\"{{ item.vlan_id }}\" provider: \"{{ nxos_provider }}\" with_items: \"{{ L2VNI }}\" tags: vxlan - name: Configure L3VNI nxos_interface: interface: vlan\"{{ L3VNI[0].vlan_id }}\" provider: \"{{ nxos_provider }}\" tags: vxlan - name: Assign interface to Tenant VRF nxos_vrf_interface: vrf: Tenant-1 interface: \"vlan{{ item.vlan_id }}\" provider: \"{{ nxos_provider }}\" with_items: - \"{{ L2VNI }}\" - \"{{ L3VNI }}\" tags: vxlan - name: Configure SVI IP nxos_ip_interface: interface: \"vlan{{ item.vlan_id }}\" addr: \"{{ item.ip_add }}\" mask: \"{{ item.mask }}\" provider: \"{{ nxos_provider }}\" with_items: \"{{ L2VNI }}\" tags: vxlan - name: Configure L2VNI SVI nxos_interface: interface: vlan\"{{ item.vlan_id }}\" fabric_forwarding_anycast_gateway: true provider: \"{{ nxos_provider }}\" with_items: \"{{ L2VNI }}\" tags: vxlan - name: Configure L3VNI SVI nxos_interface: interface: vlan\"{{ L3VNI[0].vlan_id }}\" ip_forward: enable provider: \"{{ nxos_provider }}\" tags: vxlan - name: Configure VTEP Tunnel nxos_vxlan_vtep: interface: nve1 shutdown: \"false\" source_interface: Loopback1 host_reachability: \"true\" provider: \"{{ nxos_provider }}\" tags: vxlan - name: Configure L2VNI to VTEP nxos_vxlan_vtep_vni: interface: nve1 vni: \"{{ item.vni }}\" multicast_group: \"{{ item.mcast }}\" provider: \"{{ nxos_provider }}\" with_items: \"{{ L2VNI }}\" tags: vxlan - name: Configure L3VNI to VTEP nxos_vxlan_vtep_vni: interface: nve1 vni: \"{{ L3VNI[0].vni }}\" assoc_vrf: true provider: \"{{ nxos_provider }}\" tags: vxlan Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.","title":"Edit playbook for leaf role"},{"location":"task4-vxlan-nxos/#edit-variable-file-for-leaf-role_1","text":"Use \u201cAtom\u201d to edit the main.yml file. Open up the project folder \u201cLTRDCN-1572\u201d and open \" main.yml \"\" file under \u201croles/leaf/vars/\u201d . Add the below content in the file at the end of the file (i.e., in addition to existing content), and then make sure to click \u201cFile\u201d->\u201cSave\u201d on Atom, so that the updated file is pushed to Ansible server. L2VNI: - { vlan_id: 140, vni: 50140, ip_add: 172.21.140.1, mask: 24, vlan_name: L2-VNI-140-Tenant1, mcast: 239.0.0.140 } - { vlan_id: 141, vni: 50141, ip_add: 172.21.141.1, mask: 24, vlan_name: L2-VNI-141-Tenant1, mcast: 239.0.0.141 } L3VNI: - { vlan_id: 999, vlan_name: L3-VNI-999-Tenant1, vni: 50999 } Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.","title":"Edit variable file for leaf role"},{"location":"task4-vxlan-nxos/#run-the-playbook-and-verify-configuration-changes_1","text":"On Ansible server (via SSH connection on MTputty), run the below command: [root@rhel7-tools LTRDCN-1572]# ansible-playbook nxos_fabric.yml --tags \"vxlan\" Below screenshot show partial output of above command: After finishing this task: login to any leaf switch (on MTPutty) to verify VXLAN configuration and the VNI by issuing the command: show nve vni Below screenshot shows the output of above command from Leaf-3: ![](pic/4-5-2.png)","title":"Run the playbook and verify configuration changes"},{"location":"task4-vxlan-nxos/#step-10-configure-evpn-using-ansible","text":"In this section, we will be configuring BGP EVPN on leaf and spine switches. The NXOS modules we will be using in this section are: nxos_bgp_af Manage BGP address-famility config nxos_bgp_neighbor_af Manage BGP neighbor address-famility config nxos_evpn_vni Manage Cisco EVPN VXLAN Network Identifier","title":"Step 10: Configure EVPN using Ansible"},{"location":"task4-vxlan-nxos/#edit-playbook-for-spine-role_2","text":"Use \u201cAtom\u201d to edit the main.yml file. Open up the project folder \u201cLTRDCN-1572\u201d and open \u201c main.yml \u201d file under \u201croles/spine/tasks/\u201d . Add the below content in the file in addition to existing content. Then make sure to click \u201cFile\u201d->\u201cSave\u201d on Atom, so that the updated file is pushed to Ansible server. # task to configure BGP EVPN - name: Configure BGP EVPN nxos_bgp_af: asn: \"{{ asn }}\" afi: l2vpn safi: evpn provider: \"{{ nxos_provider }}\" tags: evpn - name: Configure iBGP neighbor EVPN AF nxos_bgp_neighbor_af: asn: \"{{ asn }}\" neighbor: \"{{ item.neighbor }}\" afi: l2vpn safi: evpn route_reflector_client: \"true\" send_community: both provider: \"{{ nxos_provider }}\" with_items: \"{{ bgp_neighbors }}\" tags: evpn","title":"Edit playbook for spine role"},{"location":"task4-vxlan-nxos/#edit-variable-file-for-spine-role_2","text":"No new variables required for Spine","title":"Edit variable file for Spine role"},{"location":"task4-vxlan-nxos/#edit-playbook-for-leaf-role_2","text":"use \u201cAtom\u201d to edit the \u201c main.yml \u201d file. Open up the project folder \u201cLTRDCN-1572\u201d and open \u201c main.yml \u201d file under \u201croles/leaf/tasks/\u201d . Add the below content in the file in addition to existing content. Then make sure to click \u201cFile\u201d->\u201cSave\u201d on Atom, so that the updated file is pushed to Ansible server. #task to configure BGP EVPN - name: Configure BGP EVPN nxos_bgp_af: asn: \"{{ asn }}\" afi: l2vpn safi: evpn provider: \"{{ nxos_provider }}\" tags: evpn - name: Configure iBGP neighbor EVPN AF nxos_bgp_neighbor_af: asn: \"{{ asn }}\" neighbor: \"{{ item.neighbor }}\" afi: l2vpn safi: evpn send_community: both provider: \"{{ nxos_provider }}\" with_items: \"{{ bgp_neighbors }}\" tags: evpn - name: Configure L2VNI RD/RT nxos_evpn_vni: vni: \"{{ item.vni }}\" route_distinguisher: auto route_target_both: auto provider: \"{{ nxos_provider }}\" with_items: \"{{ L2VNI }}\" tags: evpn Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.","title":"Edit playbook for leaf role"},{"location":"task4-vxlan-nxos/#edit-variable-file-for-leaf-role_2","text":"No new variables required for Leaf","title":"Edit variable file for leaf role"},{"location":"task4-vxlan-nxos/#run-the-playbook-and-verify-configuration-changes_2","text":"On the Ansible server (using MTPutty) run this playbook by running the command ansible-playbook nxos_fabric.yml --tags \"evpn\" as shown below: [root@rhel7-tools LTRDCN-1572]# ansible-playbook nxos_fabric.yml --tags \"evpn\" Below screenshot shows the output of above playbook: After successful execution of the playbook: login to any leaf or spine switch to verify BGP EVPN configuration and evpn neighbor by issuing command: show bgp l2vpn evpn summary Below screenshot shows the output of above command from leaf-3 switch. As expected it shows the spine-1 and spine-2 as neighbors:","title":"Run the playbook and verify configuration changes"},{"location":"task4-vxlan-nxos/#step-11-run-nxos_fabric-playbook","text":"Up to this point, you have run the playbook for each step separately (using tags). You could re-run the whole playbook without giving any tags, but no new changes should be maded to the switches. [root@rhel7-tools LTRDCN-1572]# ansible-playbook nxos_fabric.yml","title":"Step 11: Run nxos_fabric playbook"},{"location":"task4-vxlan-nxos/#step-12-verify-end-to-end-ip-connectivity","text":"Now Let\u2019s verify the VXLAN bridging and VXLAN routing from servers that are pre-configured in following VLANs and IPs Server Name Connect to switch In VLAN IP of server Server-1 Leaf-1 140 172.21.140.10 Server-3 Leaf-3 140 172.21.140.11 Server-4 Leaf-4 141 172.21.141.11 Below figure shows the topology & connectivity of servers to Leaf switches and their respective IP addresses: Switch to MTPuTTY and SSH to server-1 . If prompted enter the credentials of root/C1sco12345 Ping default gateway from server-1 by issuing command ping 172.21.140.1 as shown below: [root@server-1 ~]# ping 172.21.140.1 PING 172.21.140.1 (172.21.140.1) 56(84) bytes of data. 64 bytes from 172.21.140.1: icmp_seq=2 ttl=255 time=15.7 ms 64 bytes from 172.21.140.1: icmp_seq=3 ttl=255 time=4.11 ms Next, Ping server 3 and server 4 from server-1 (in same VLAN and inter-VLAN respectively) by using ping 172.21.140.11 and ping 172.21.141.11 commands as shown below: [[root@server-1 ~]# ping 172.21.140.11 PING 172.21.140.11 (172.21.140.11) 56(84) bytes of data. 64 bytes from 172.21.140.11: icmp_seq=1 ttl=64 time=1032 ms 64 bytes from 172.21.140.11: icmp_seq=2 ttl=64 time=35.7 ms 64 bytes from 172.21.140.11: icmp_seq=3 ttl=64 time=14.4 ms ^C --- 172.21.140.11 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2112ms rtt min/avg/max/mdev = 14.431/360.839/1032.335/474.899 ms, pipe 2 [root@server-1 ~]# ping 172.21.141.11 PING 172.21.141.11 (172.21.141.11) 56(84) bytes of data. 64 bytes from 172.21.141.11: icmp_seq=994 ttl=62 time=30.2 ms 64 bytes from 172.21.141.11: icmp_seq=995 ttl=62 time=16.1 ms 64 bytes from 172.21.141.11: icmp_seq=996 ttl=62 time=18.0 ms Congratulation! You have successfuly built VXLAN fabric using ansible + Jinja2 template and ansible + NXOS modules.","title":"Step 12: Verify end-to-end IP connectivity"},{"location":"task5-day2-operation/","text":"Task 5: Day 2 operation using Ansible In this section, we will use automation to perform following day 2 operation tasks. Backup running configurations on all leaf and spine switches Verify underlay ospf, bgp and pim neighbors Verify overlay nve peer, host route, bgp update Baseline configuration comparison Add new VNIs into the existing fabric Step 1: Backup running configurations In this section, you will use ios_config module to backup running configuration on each switch, the backup file will be saved to a local \u201cbackup\u201d folder. The backup argument create a full backup of the current running-config of each switch. The backup file is written to the backup folder in the playbook root directory. If the directory does not exist, it is created. On Atom , open up the project folder \u201cLTRDCN-1572\u201d and create new file under LTRDCN-1572 . Name the new file get_config.yml . Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. --- - hosts: spine,leaf,jinja2_leaf,jinja2_spine connection: local vars: ios_provider: transport: nxapi username: \"{{ user }}\" password: \"{{ pwd }}\" host: \"{{ inventory_hostname }}\" tasks: - name: save running nxos_config: provider: \"{{ ios_provider }}\" backup: yes timeout: 20 On the Ansible node (using MTputty via SSH), run \u201cget_config.yml\u201d playbook and verify the backup configurations in \u201cbackup\u201d folder by using below commands: [root@rhel7-tools LTRDCN-1572]# ansible-playbook get_config.yml [root@rhel7-tools LTRDCN-1572]# ls -lrt [root@rhel7-tools LTRDCN-1572]# ls backup You may further view the contents of the files under backup folder by using cat, less or more commands. Below screenshot shows the output of above commands Step 2: Verify underlay and overlay In this step, you will verify underlay and overlay operation using ansible playbook. The playbook will be applied to all leaf switches to verify the below commands: Underlay - show ip ospf neighbor - show ip bgp sum - show ip pim neighbor Overlay - show nve vni - show nve peer - show ip route vrf Tenant-1 - show bgp l2vpn evpn - show l2route evpn mac-ip all Switch to \u201cAtom\u201d, right click on the folder LTRDDCN-1572 and create a new playbook named verify_fabric.yml . Enter this file name and hit enter. --- - hosts: leaf, jinja2_leaf connection: local gather_facts: false vars: ios_provider: username: \"{{ user }}\" password: \"{{ pwd }}\" timeout: 30 host: \"{{ inventory_hostname }}\" tasks: - name: verify underlay register: underlay_output ios_command: provider: \"{{ ios_provider }}\" commands: - show ip ospf neighbors - show ip bgp sum - show ip pim neighbor tags: underlay - debug: var=underlay_output.stdout_lines tags: underlay # - copy: content=\"{{underlay_output | to_nice_json}}\" dest=\"verify/{{inventory_hostname}}_underlay\" - name: Verify Overlay register: overlay_output ios_command: provider: \"{{ ios_provider }}\" commands: - show nve vni - show nve peer - show ip route vrf Tenant-1 - show bgp l2vpn evpn - show l2route evpn mac-ip all tags: overlay - debug: var=overlay_output.stdout_lines tags: overlay Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. On the Ansible node (via MTPutty), run verify_fabric.yml playbook and verify the output for underlay by executing below command (using respective tag): [root@rhel7-tools LTRDCN-1572]# ansible-playbook verify_fabric.yml --tags \"underlay\" The output shows ospf, bgp and pim neighbors for all leaf switches Below screenshot shows the partial output of above command: Here is a log of execution of above command: [root@rhel7-tools LTRDCN-1572]# ansible-playbook verify_fabric.yml --tags \"underlay\" PLAY [leaf, jinja2_leaf] ************************************************************************************************************************************************ TASK [verify underlay] ************************************************************************************************************************************************** [WARNING]: argument username has been deprecated and will be removed in a future version [WARNING]: argument timeout has been deprecated and will be removed in a future version [WARNING]: argument password has been deprecated and will be removed in a future version ok: [198.18.4.101] ok: [198.18.4.104] ok: [198.18.4.103] TASK [debug] ************************************************************************************************************************************************************ ok: [198.18.4.101] => { \"underlay_output.stdout_lines\": [ [ \"OSPF Process ID 1 VRF default\", \" Total number of neighbors: 2\", \" Neighbor ID Pri State Up Time Address Interface\", \" 192.168.0.6 1 FULL/ - 1d03h 10.0.0.21 Eth1/1 \", \" 192.168.0.7 1 FULL/ - 1d03h 10.0.128.5 Eth1/2\" ], [ \"BGP summary information for VRF default, address family IPv4 Unicast\", \"BGP router identifier 192.168.0.8, local AS number 65000\", \"BGP table version is 8, IPv4 Unicast config peers 2, capable peers 2\", \"0 network entries and 0 paths using 0 bytes of memory\", \"BGP attribute entries [0/0], BGP AS path entries [0/0]\", \"BGP community entries [0/0], BGP clusterlist entries [0/0]\", \"\", \"Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd\", \"192.168.0.6 4 65000 52 54 8 0 0 00:24:18 0 \", \"192.168.0.7 4 65000 47 49 8 0 0 00:30:45 0\" ], [ \"PIM Neighbor Status for VRF \\\"default\\\"\", \"Neighbor Interface Uptime Expires DR Bidir- BFD\", \" Priority Capable State\", \"10.0.0.21 Ethernet1/1 00:15:51 00:01:43 1 yes n/a\", \"10.0.128.5 Ethernet1/2 00:15:45 00:01:23 1 yes n/a\" ] ] } ok: [198.18.4.104] => { \"underlay_output.stdout_lines\": [ [ \"OSPF Process ID 1 VRF default\", \" Total number of neighbors: 2\", \" Neighbor ID Pri State Up Time Address Interface\", \" 192.168.0.6 1 FULL/ - 10:20:18 10.0.128.1 Eth1/1 \", \" 192.168.0.7 1 FULL/ - 10:20:15 10.0.128.17 Eth1/2\" ], [ \"BGP summary information for VRF default, address family IPv4 Unicast\", \"BGP router identifier 192.168.0.11, local AS number 65000\", \"BGP table version is 4, IPv4 Unicast config peers 2, capable peers 2\", \"0 network entries and 0 paths using 0 bytes of memory\", \"BGP attribute entries [0/0], BGP AS path entries [0/0]\", \"BGP community entries [0/0], BGP clusterlist entries [0/0]\", \"\", \"Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd\", \"192.168.0.6 4 65000 32 32 4 0 0 00:24:03 0 \", \"192.168.0.7 4 65000 34 34 4 0 0 00:25:16 0\" ], [ \"PIM Neighbor Status for VRF \\\"default\\\"\", \"Neighbor Interface Uptime Expires DR Bidir- BFD\", \" Priority Capable State\", \"10.0.128.1 Ethernet1/1 00:19:15 00:01:31 1 yes n/a\", \"10.0.128.17 Ethernet1/2 00:25:43 00:01:18 1 yes n/a\" ] ] } ok: [198.18.4.103] => { \"underlay_output.stdout_lines\": [ [ \"OSPF Process ID 1 VRF default\", \" Total number of neighbors: 2\", \" Neighbor ID Pri State Up Time Address Interface\", \" 192.168.0.6 1 FULL/ - 1d03h 10.0.0.29 Eth1/1 \", \" 192.168.0.7 1 FULL/ - 1d03h 10.0.128.13 Eth1/2\" ], [ \"BGP summary information for VRF default, address family IPv4 Unicast\", \"BGP router identifier 192.168.0.10, local AS number 65000\", \"BGP table version is 8, IPv4 Unicast config peers 2, capable peers 2\", \"0 network entries and 0 paths using 0 bytes of memory\", \"BGP attribute entries [0/0], BGP AS path entries [0/0]\", \"BGP community entries [0/0], BGP clusterlist entries [0/0]\", \"\", \"Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd\", \"192.168.0.6 4 65000 52 53 8 0 0 00:24:19 0 \", \"192.168.0.7 4 65000 47 49 8 0 0 00:30:47 0\" ], [ \"PIM Neighbor Status for VRF \\\"default\\\"\", \"Neighbor Interface Uptime Expires DR Bidir- BFD\", \" Priority Capable State\", \"10.0.0.29 Ethernet1/1 00:15:53 00:01:23 1 yes n/a\", \"10.0.128.13 Ethernet1/2 00:15:47 00:01:37 1 yes n/a\" ] ] } PLAY RECAP ************************************************************************************************************************************************************** 198.18.4.101 : ok=2 changed=0 unreachable=0 failed=0 198.18.4.103 : ok=2 changed=0 unreachable=0 failed=0 198.18.4.104 : ok=2 changed=0 unreachable=0 failed=0 Next: Run verify_fabric.yml playbook and verify the output for overlay using the respective tag in the command (as shown below): [root@rhel7-tools LTRDCN-1572]# ansible-playbook verify_fabric.yml --tags \"overlay\" The output shows nve tunnel peer, host route in bgp EVPN from all leaf switches Below screenshot of the partial output of above command: Below shows the complete log output of execution of above playbook command. Verify the output for vne vni status, vne dynamic neighbors, type host mac+ip evpn route update for each L2VNI, l2fib information. [root@rhel7-tools LTRDCN-1572]# ansible-playbook verify_fabric.yml --tags \"overlay\" PLAY [leaf, jinja2_leaf] ********************************************************************************************************************************************** TASK [Verify Overlay] ************************************************************************************************************************************************* [WARNING]: argument username has been deprecated and will be removed in a future version [WARNING]: argument timeout has been deprecated and will be removed in a future version [WARNING]: argument password has been deprecated and will be removed in a future version ok: [198.18.4.101] ok: [198.18.4.104] ok: [198.18.4.103] TASK [debug] ********************************************************************************************************************************************************** ok: [198.18.4.101] => { \"overlay_output.stdout_lines\": [ [ \"Codes: CP - Control Plane DP - Data Plane \", \" UC - Unconfigured SA - Suppress ARP \", \" SU - Suppress Unknown Unicast\", \" \", \"Interface VNI Multicast-group State Mode Type [BD/VRF] Flags\", \"--------- -------- ----------------- ----- ---- ------------------ -----\", \"nve1 50140 239.0.0.140 Up CP L2 [140] \", \"nve1 50141 239.0.0.141 Up CP L2 [141] \", \"nve1 50999 n/a Up CP L3 [Tenant-1]\" ], [ \"Interface Peer-IP State LearnType Uptime Router-Mac \", \"--------- --------------- ----- --------- -------- -----------------\", \"nve1 192.168.0.110 Up CP 00:03:19 000c.2939.f53f \", \"nve1 192.168.0.111 Up CP 00:01:12 000c.2951.176f\" ], [ \"IP Route Table for VRF \\\"Tenant-1\\\"\", \"'*' denotes best ucast next-hop\", \"'**' denotes best mcast next-hop\", \"'[x/y]' denotes [preference/metric]\", \"'%<string>' in via output denotes VRF <string>\", \"\", \"172.21.140.0/24, ubest/mbest: 1/0, attached\", \" *via 172.21.140.1, Vlan140, [0/0], 00:05:39, direct\", \"172.21.140.1/32, ubest/mbest: 1/0, attached\", \" *via 172.21.140.1, Vlan140, [0/0], 00:05:39, local\", \"172.21.140.10/32, ubest/mbest: 1/0, attached\", \" *via 172.21.140.10, Vlan140, [190/0], 00:05:33, hmm\", \"172.21.140.11/32, ubest/mbest: 1/0\", \" *via 192.168.0.110%default, [200/0], 00:01:50, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a8006e encap: VXLAN\", \" \", \"172.21.141.0/24, ubest/mbest: 1/0, attached\", \" *via 172.21.141.1, Vlan141, [0/0], 00:05:38, direct\", \"172.21.141.1/32, ubest/mbest: 1/0, attached\", \" *via 172.21.141.1, Vlan141, [0/0], 00:05:38, local\", \"172.21.141.11/32, ubest/mbest: 1/0\", \" *via 192.168.0.111%default, [200/0], 00:01:12, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a8006f encap: VXLAN\" ], [ \"BGP routing table information for VRF default, address family L2VPN EVPN\", \"BGP table version is 35, Local Router ID is 192.168.0.8\", \"Status: s-suppressed, x-deleted, S-stale, d-dampened, h-history, *-valid, >-best\", \"Path type: i-internal, e-external, c-confed, l-local, a-aggregate, r-redist, I-injected\", \"Origin codes: i - IGP, e - EGP, ? - incomplete, | - multipath, & - backup\", \"\", \" Network Next Hop Metric LocPrf Weight Path\", \"Route Distinguisher: 192.168.0.8:32907 (L2VNI 50140)\", \"*>i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216\", \" 192.168.0.110 100 0 i\", \"*>l[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\", \" 192.168.0.18 100 32768 i\", \"*>i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\", \" 192.168.0.110 100 0 i\", \"\", \"Route Distinguisher: 192.168.0.8:32908 (L2VNI 50141)\", \"*>i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216\", \" 192.168.0.111 100 0 i\", \"*>i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\", \" 192.168.0.111 100 0 i\", \"\", \"Route Distinguisher: 192.168.0.10:32907\", \"* i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216\", \" 192.168.0.110 100 0 i\", \"*>i 192.168.0.110 100 0 i\", \"*>i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\", \" 192.168.0.110 100 0 i\", \"* i 192.168.0.110 100 0 i\", \"\", \"Route Distinguisher: 192.168.0.11:32908\", \"* i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216\", \" 192.168.0.111 100 0 i\", \"*>i 192.168.0.111 100 0 i\", \"*>i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\", \" 192.168.0.111 100 0 i\", \"* i 192.168.0.111 100 0 i\", \"\", \"Route Distinguisher: 192.168.0.8:3 (L3VNI 50999)\", \"*>i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\", \" 192.168.0.111 100 0 i\", \"*>i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\", \" 192.168.0.110 100 0 i\" ], [ \"Flags -(Rmac):Router MAC (Stt):Static (L):Local (R):Remote (V):vPC link \", \"(Dup):Duplicate (Spl):Split (Rcv):Recv(D):Del Pending (S):Stale (C):Clear\", \"(Ps):Peer Sync (Ro):Re-Originated \", \"Topology Mac Address Prod Flags Seq No Host IP Next-Hops \", \"----------- -------------- ------ ---------- --------------- ---------------\", \"140 0050.56a0.7630 HMM -- 0 172.21.140.10 Local \", \"140 0050.56a0.b5d1 BGP -- 0 172.21.140.11 192.168.0.110 \", \"141 000c.2979.f00d BGP -- 1 172.21.141.11 192.168.0.111\" ] ] } Step 3: Baseline configuration comparison In this section we will compare the running configuration with baseline configuration for configuration compliance check. The configuration file that we backed in tak 1 will be used as baseline configuration. In this playbook, you will use \u201clookup\u201d module to find the backup filename generated in Step 1. Then you will use diff_against function in nxos_config module to compare running configuration. On Atom, Open up the project folder LTRDCN-1572 and create new file under \u201cLTRDCN-1572\u201d . Name the new file verify_config.yml and enter below data in this playbook: --- - hosts: jinja2_leaf,leaf,jinja2_spine,spine connection: local gather_facts: flase vars: nxos_provider: transport: nxapi username: \"{{ user }}\" password: \"{{ pwd }}\" timeout: 30 host: \"{{ inventory_hostname }}\" filename: \"{{ lookup('pipe', 'ls backup/{{ inventory_hostname}}_config.*')}}\" tasks: - name: configure compliance register: diff_config # when: (inventory_hostname in groups['leaf']) or (inventory_hostname in groups['jinja2_leaf']) nxos_config: provider: \"{{ nxos_provider }}\" diff_against: intended intended_config: \"{{ lookup('file', '{{filename}}') }}\" Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. Before you run this playbook, SSH into leaf-4 to make some configuration changes by issuing below commands: config t no router bgp 65000 copy run start end Here is a log of execution of above command: Leaf-4# conf t Enter configuration commands, one per line. End with CNTL/Z. Leaf-4(config)# no router bgp 65000 Leaf-4(config)# copy run start [########################################] 100% Leaf-4(config)# end Leaf-4# On the Ansible server (via MTputty SSH session), run the playbook for configuration compliance check by executing ansible-playbook --diff verify_config.yml as shown below below: [root@rhel7-tools LTRDCN-1572]# ansible-playbook --diff verify_config.yml The delta between current running config and base line config are highlighted in RED from the result Below partial screenshot shows the output of above command: Bring leaf-4 back to the baseline config by executing ansible-playbook jinja2_fabric.yml --limit=198.18.4.104 command as shown below: [root@rhel7-tools LTRDCN-1572]# ansible-playbook jinja2_fabric.yml --limit=198.18.4.104 Below screenshot shows the output of above command. You can also log into leaf-4 and verify that bgp configurations are back: Step 4: Add new VNI In this section, we will introduce following new VNI into the VXLAN fabric. VLAN ID VLAN Name VNI IP_Add mask Mcast 200 L2-VNI-200-Tenant1 50200 172.21.200.1 24 239.0.0.200 201 L2-VNI-201-Tenant1 50201 172.21.201.1 24 239.0.0.201 First we will creat a new role, and name it \u201cvni_provision\u201d under folder roles using ansible-galaxy using below commands on the Ansible node (using MTputty via SSH connection): [root@rhel7-tools LTRDCN-1572]# cd roles/ [root@rhel7-tools roles]# ansible-galaxy init vni_provision Verify vni_provision was created successfully Ansible-galaxy init will create new role with base role structure and empty main.yml file as role requires. Switch to \u201cAtom\u201d and sync the new created folders between Ansible node and remote desktop. Right click on project folder \u201cLTRDCN-1572\u201d , open \u201cRemote Sync\u201d select \u201cDownload Folder\u201d Edit variable file main.yml for \u201cvni_provision\u201d role under \u201c /root/LTRDCN-1572/roles/vni_provision/vars \u201d and enter below data. Make sure to click File and Save on Atom to push this to Ansible server: --- # vars file for vni_provision nxos_provider: username: \"{{ user }}\" password: \"{{ pwd }}\" transport: nxapi timeout: 30 host: \"{{ inventory_hostname }}\" L2VNI: - { vlan_id: 200, vni: 50200, ip_add: 172.21.200.1, mask: 24, vlan_name: L2-VNI-200-Tenant1, mcast: 239.0.0.200 } - { vlan_id: 201, vni: 50201, ip_add: 172.21.201.1, mask: 24, vlan_name: L2-VNI-201-Tenant1, mcast: 239.0.0.201 } Edit playbook file main.yml for \u201cvni_provision\u201d role under \u201c/root/LTRDCN-1572/roles/vni_provision/tasks\u201d and enter below data. Make sure to click File and Save on Atom to push this to Ansible server: --- # tasks file for vni_provision - name: Configure VLAN to VNI nxos_vlan: vlan_id: \"{{ item.vlan_id }}\" mapped_vni: \"{{ item.vni }}\" name: \"{{ item.vlan_name }}\" provider: \"{{ nxos_provider }}\" with_items: - \"{{ L2VNI }}\" - name: Configure L2VNI nxos_interface: interface: vlan\"{{ item.vlan_id }}\" provider: \"{{ nxos_provider }}\" with_items: \"{{ L2VNI }}\" - name: Assign interface to Tenant VRF nxos_vrf_interface: vrf: Tenant-1 interface: \"vlan{{ item.vlan_id }}\" provider: \"{{ nxos_provider }}\" with_items: - \"{{ L2VNI }}\" - name: Configure SVI IP nxos_ip_interface: interface: \"vlan{{ item.vlan_id }}\" addr: \"{{ item.ip_add }}\" mask: \"{{ item.mask }}\" provider: \"{{ nxos_provider }}\" with_items: \"{{ L2VNI }}\" - name: Configure L2VNI SVI nxos_interface: interface: vlan\"{{ item.vlan_id }}\" fabric_forwarding_anycast_gateway: true provider: \"{{ nxos_provider }}\" with_items: \"{{ L2VNI }}\" - name: Configure L2VNI to VTEP nxos_vxlan_vtep_vni: interface: nve1 vni: \"{{ item.vni }}\" multicast_group: \"{{ item.mcast }}\" provider: \"{{ nxos_provider }}\" with_items: \"{{ L2VNI }}\" - name: Configure L2VNI RD/RT nxos_evpn_vni: vni: \"{{ item.vni }}\" route_distinguisher: auto route_target_both: auto provider: \"{{ nxos_provider }}\" with_items: \"{{ L2VNI }}\" this is shown in below screenshot: Switch to \u201cAtom\u201d create new playbook \u2018vni_provision.yml\u2019 under project folder LTRDCN-1572 and enter below data. Make sure to click File and Save on Atom to push this to Ansible server: --- - hosts: leaf,jinja2_leaf connection: local roles: - vni_provision Run playbook vni_provision.yml to add new VNIs on the fabric by issuing ansible-playbook vni_provision.yml command as shown below: [root@rhel7-tools LTRDCN-1572]# ansible-playbook vni_provision.yml Below screenshot shows the output of above command: Switch to MTPutty and connect to leaf-4 (SSH connection0, verify the change on leaf switches by issuing below command: show nve vni Notice the new created L2VNI as shown in below screenshot: Congratulation! You have completed VXLAN Fabric Lab.","title":"Task 5 - Day 2 Operations"},{"location":"task5-day2-operation/#task-5-day-2-operation-using-ansible","text":"In this section, we will use automation to perform following day 2 operation tasks. Backup running configurations on all leaf and spine switches Verify underlay ospf, bgp and pim neighbors Verify overlay nve peer, host route, bgp update Baseline configuration comparison Add new VNIs into the existing fabric","title":"Task 5: Day 2 operation using Ansible"},{"location":"task5-day2-operation/#step-1-backup-running-configurations","text":"In this section, you will use ios_config module to backup running configuration on each switch, the backup file will be saved to a local \u201cbackup\u201d folder. The backup argument create a full backup of the current running-config of each switch. The backup file is written to the backup folder in the playbook root directory. If the directory does not exist, it is created. On Atom , open up the project folder \u201cLTRDCN-1572\u201d and create new file under LTRDCN-1572 . Name the new file get_config.yml . Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. --- - hosts: spine,leaf,jinja2_leaf,jinja2_spine connection: local vars: ios_provider: transport: nxapi username: \"{{ user }}\" password: \"{{ pwd }}\" host: \"{{ inventory_hostname }}\" tasks: - name: save running nxos_config: provider: \"{{ ios_provider }}\" backup: yes timeout: 20 On the Ansible node (using MTputty via SSH), run \u201cget_config.yml\u201d playbook and verify the backup configurations in \u201cbackup\u201d folder by using below commands: [root@rhel7-tools LTRDCN-1572]# ansible-playbook get_config.yml [root@rhel7-tools LTRDCN-1572]# ls -lrt [root@rhel7-tools LTRDCN-1572]# ls backup You may further view the contents of the files under backup folder by using cat, less or more commands. Below screenshot shows the output of above commands","title":"Step 1: Backup running configurations"},{"location":"task5-day2-operation/#step-2-verify-underlay-and-overlay","text":"In this step, you will verify underlay and overlay operation using ansible playbook. The playbook will be applied to all leaf switches to verify the below commands: Underlay - show ip ospf neighbor - show ip bgp sum - show ip pim neighbor Overlay - show nve vni - show nve peer - show ip route vrf Tenant-1 - show bgp l2vpn evpn - show l2route evpn mac-ip all Switch to \u201cAtom\u201d, right click on the folder LTRDDCN-1572 and create a new playbook named verify_fabric.yml . Enter this file name and hit enter. --- - hosts: leaf, jinja2_leaf connection: local gather_facts: false vars: ios_provider: username: \"{{ user }}\" password: \"{{ pwd }}\" timeout: 30 host: \"{{ inventory_hostname }}\" tasks: - name: verify underlay register: underlay_output ios_command: provider: \"{{ ios_provider }}\" commands: - show ip ospf neighbors - show ip bgp sum - show ip pim neighbor tags: underlay - debug: var=underlay_output.stdout_lines tags: underlay # - copy: content=\"{{underlay_output | to_nice_json}}\" dest=\"verify/{{inventory_hostname}}_underlay\" - name: Verify Overlay register: overlay_output ios_command: provider: \"{{ ios_provider }}\" commands: - show nve vni - show nve peer - show ip route vrf Tenant-1 - show bgp l2vpn evpn - show l2route evpn mac-ip all tags: overlay - debug: var=overlay_output.stdout_lines tags: overlay Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. On the Ansible node (via MTPutty), run verify_fabric.yml playbook and verify the output for underlay by executing below command (using respective tag): [root@rhel7-tools LTRDCN-1572]# ansible-playbook verify_fabric.yml --tags \"underlay\" The output shows ospf, bgp and pim neighbors for all leaf switches Below screenshot shows the partial output of above command: Here is a log of execution of above command: [root@rhel7-tools LTRDCN-1572]# ansible-playbook verify_fabric.yml --tags \"underlay\" PLAY [leaf, jinja2_leaf] ************************************************************************************************************************************************ TASK [verify underlay] ************************************************************************************************************************************************** [WARNING]: argument username has been deprecated and will be removed in a future version [WARNING]: argument timeout has been deprecated and will be removed in a future version [WARNING]: argument password has been deprecated and will be removed in a future version ok: [198.18.4.101] ok: [198.18.4.104] ok: [198.18.4.103] TASK [debug] ************************************************************************************************************************************************************ ok: [198.18.4.101] => { \"underlay_output.stdout_lines\": [ [ \"OSPF Process ID 1 VRF default\", \" Total number of neighbors: 2\", \" Neighbor ID Pri State Up Time Address Interface\", \" 192.168.0.6 1 FULL/ - 1d03h 10.0.0.21 Eth1/1 \", \" 192.168.0.7 1 FULL/ - 1d03h 10.0.128.5 Eth1/2\" ], [ \"BGP summary information for VRF default, address family IPv4 Unicast\", \"BGP router identifier 192.168.0.8, local AS number 65000\", \"BGP table version is 8, IPv4 Unicast config peers 2, capable peers 2\", \"0 network entries and 0 paths using 0 bytes of memory\", \"BGP attribute entries [0/0], BGP AS path entries [0/0]\", \"BGP community entries [0/0], BGP clusterlist entries [0/0]\", \"\", \"Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd\", \"192.168.0.6 4 65000 52 54 8 0 0 00:24:18 0 \", \"192.168.0.7 4 65000 47 49 8 0 0 00:30:45 0\" ], [ \"PIM Neighbor Status for VRF \\\"default\\\"\", \"Neighbor Interface Uptime Expires DR Bidir- BFD\", \" Priority Capable State\", \"10.0.0.21 Ethernet1/1 00:15:51 00:01:43 1 yes n/a\", \"10.0.128.5 Ethernet1/2 00:15:45 00:01:23 1 yes n/a\" ] ] } ok: [198.18.4.104] => { \"underlay_output.stdout_lines\": [ [ \"OSPF Process ID 1 VRF default\", \" Total number of neighbors: 2\", \" Neighbor ID Pri State Up Time Address Interface\", \" 192.168.0.6 1 FULL/ - 10:20:18 10.0.128.1 Eth1/1 \", \" 192.168.0.7 1 FULL/ - 10:20:15 10.0.128.17 Eth1/2\" ], [ \"BGP summary information for VRF default, address family IPv4 Unicast\", \"BGP router identifier 192.168.0.11, local AS number 65000\", \"BGP table version is 4, IPv4 Unicast config peers 2, capable peers 2\", \"0 network entries and 0 paths using 0 bytes of memory\", \"BGP attribute entries [0/0], BGP AS path entries [0/0]\", \"BGP community entries [0/0], BGP clusterlist entries [0/0]\", \"\", \"Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd\", \"192.168.0.6 4 65000 32 32 4 0 0 00:24:03 0 \", \"192.168.0.7 4 65000 34 34 4 0 0 00:25:16 0\" ], [ \"PIM Neighbor Status for VRF \\\"default\\\"\", \"Neighbor Interface Uptime Expires DR Bidir- BFD\", \" Priority Capable State\", \"10.0.128.1 Ethernet1/1 00:19:15 00:01:31 1 yes n/a\", \"10.0.128.17 Ethernet1/2 00:25:43 00:01:18 1 yes n/a\" ] ] } ok: [198.18.4.103] => { \"underlay_output.stdout_lines\": [ [ \"OSPF Process ID 1 VRF default\", \" Total number of neighbors: 2\", \" Neighbor ID Pri State Up Time Address Interface\", \" 192.168.0.6 1 FULL/ - 1d03h 10.0.0.29 Eth1/1 \", \" 192.168.0.7 1 FULL/ - 1d03h 10.0.128.13 Eth1/2\" ], [ \"BGP summary information for VRF default, address family IPv4 Unicast\", \"BGP router identifier 192.168.0.10, local AS number 65000\", \"BGP table version is 8, IPv4 Unicast config peers 2, capable peers 2\", \"0 network entries and 0 paths using 0 bytes of memory\", \"BGP attribute entries [0/0], BGP AS path entries [0/0]\", \"BGP community entries [0/0], BGP clusterlist entries [0/0]\", \"\", \"Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd\", \"192.168.0.6 4 65000 52 53 8 0 0 00:24:19 0 \", \"192.168.0.7 4 65000 47 49 8 0 0 00:30:47 0\" ], [ \"PIM Neighbor Status for VRF \\\"default\\\"\", \"Neighbor Interface Uptime Expires DR Bidir- BFD\", \" Priority Capable State\", \"10.0.0.29 Ethernet1/1 00:15:53 00:01:23 1 yes n/a\", \"10.0.128.13 Ethernet1/2 00:15:47 00:01:37 1 yes n/a\" ] ] } PLAY RECAP ************************************************************************************************************************************************************** 198.18.4.101 : ok=2 changed=0 unreachable=0 failed=0 198.18.4.103 : ok=2 changed=0 unreachable=0 failed=0 198.18.4.104 : ok=2 changed=0 unreachable=0 failed=0 Next: Run verify_fabric.yml playbook and verify the output for overlay using the respective tag in the command (as shown below): [root@rhel7-tools LTRDCN-1572]# ansible-playbook verify_fabric.yml --tags \"overlay\" The output shows nve tunnel peer, host route in bgp EVPN from all leaf switches Below screenshot of the partial output of above command: Below shows the complete log output of execution of above playbook command. Verify the output for vne vni status, vne dynamic neighbors, type host mac+ip evpn route update for each L2VNI, l2fib information. [root@rhel7-tools LTRDCN-1572]# ansible-playbook verify_fabric.yml --tags \"overlay\" PLAY [leaf, jinja2_leaf] ********************************************************************************************************************************************** TASK [Verify Overlay] ************************************************************************************************************************************************* [WARNING]: argument username has been deprecated and will be removed in a future version [WARNING]: argument timeout has been deprecated and will be removed in a future version [WARNING]: argument password has been deprecated and will be removed in a future version ok: [198.18.4.101] ok: [198.18.4.104] ok: [198.18.4.103] TASK [debug] ********************************************************************************************************************************************************** ok: [198.18.4.101] => { \"overlay_output.stdout_lines\": [ [ \"Codes: CP - Control Plane DP - Data Plane \", \" UC - Unconfigured SA - Suppress ARP \", \" SU - Suppress Unknown Unicast\", \" \", \"Interface VNI Multicast-group State Mode Type [BD/VRF] Flags\", \"--------- -------- ----------------- ----- ---- ------------------ -----\", \"nve1 50140 239.0.0.140 Up CP L2 [140] \", \"nve1 50141 239.0.0.141 Up CP L2 [141] \", \"nve1 50999 n/a Up CP L3 [Tenant-1]\" ], [ \"Interface Peer-IP State LearnType Uptime Router-Mac \", \"--------- --------------- ----- --------- -------- -----------------\", \"nve1 192.168.0.110 Up CP 00:03:19 000c.2939.f53f \", \"nve1 192.168.0.111 Up CP 00:01:12 000c.2951.176f\" ], [ \"IP Route Table for VRF \\\"Tenant-1\\\"\", \"'*' denotes best ucast next-hop\", \"'**' denotes best mcast next-hop\", \"'[x/y]' denotes [preference/metric]\", \"'%<string>' in via output denotes VRF <string>\", \"\", \"172.21.140.0/24, ubest/mbest: 1/0, attached\", \" *via 172.21.140.1, Vlan140, [0/0], 00:05:39, direct\", \"172.21.140.1/32, ubest/mbest: 1/0, attached\", \" *via 172.21.140.1, Vlan140, [0/0], 00:05:39, local\", \"172.21.140.10/32, ubest/mbest: 1/0, attached\", \" *via 172.21.140.10, Vlan140, [190/0], 00:05:33, hmm\", \"172.21.140.11/32, ubest/mbest: 1/0\", \" *via 192.168.0.110%default, [200/0], 00:01:50, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a8006e encap: VXLAN\", \" \", \"172.21.141.0/24, ubest/mbest: 1/0, attached\", \" *via 172.21.141.1, Vlan141, [0/0], 00:05:38, direct\", \"172.21.141.1/32, ubest/mbest: 1/0, attached\", \" *via 172.21.141.1, Vlan141, [0/0], 00:05:38, local\", \"172.21.141.11/32, ubest/mbest: 1/0\", \" *via 192.168.0.111%default, [200/0], 00:01:12, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a8006f encap: VXLAN\" ], [ \"BGP routing table information for VRF default, address family L2VPN EVPN\", \"BGP table version is 35, Local Router ID is 192.168.0.8\", \"Status: s-suppressed, x-deleted, S-stale, d-dampened, h-history, *-valid, >-best\", \"Path type: i-internal, e-external, c-confed, l-local, a-aggregate, r-redist, I-injected\", \"Origin codes: i - IGP, e - EGP, ? - incomplete, | - multipath, & - backup\", \"\", \" Network Next Hop Metric LocPrf Weight Path\", \"Route Distinguisher: 192.168.0.8:32907 (L2VNI 50140)\", \"*>i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216\", \" 192.168.0.110 100 0 i\", \"*>l[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\", \" 192.168.0.18 100 32768 i\", \"*>i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\", \" 192.168.0.110 100 0 i\", \"\", \"Route Distinguisher: 192.168.0.8:32908 (L2VNI 50141)\", \"*>i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216\", \" 192.168.0.111 100 0 i\", \"*>i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\", \" 192.168.0.111 100 0 i\", \"\", \"Route Distinguisher: 192.168.0.10:32907\", \"* i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216\", \" 192.168.0.110 100 0 i\", \"*>i 192.168.0.110 100 0 i\", \"*>i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\", \" 192.168.0.110 100 0 i\", \"* i 192.168.0.110 100 0 i\", \"\", \"Route Distinguisher: 192.168.0.11:32908\", \"* i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216\", \" 192.168.0.111 100 0 i\", \"*>i 192.168.0.111 100 0 i\", \"*>i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\", \" 192.168.0.111 100 0 i\", \"* i 192.168.0.111 100 0 i\", \"\", \"Route Distinguisher: 192.168.0.8:3 (L3VNI 50999)\", \"*>i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\", \" 192.168.0.111 100 0 i\", \"*>i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\", \" 192.168.0.110 100 0 i\" ], [ \"Flags -(Rmac):Router MAC (Stt):Static (L):Local (R):Remote (V):vPC link \", \"(Dup):Duplicate (Spl):Split (Rcv):Recv(D):Del Pending (S):Stale (C):Clear\", \"(Ps):Peer Sync (Ro):Re-Originated \", \"Topology Mac Address Prod Flags Seq No Host IP Next-Hops \", \"----------- -------------- ------ ---------- --------------- ---------------\", \"140 0050.56a0.7630 HMM -- 0 172.21.140.10 Local \", \"140 0050.56a0.b5d1 BGP -- 0 172.21.140.11 192.168.0.110 \", \"141 000c.2979.f00d BGP -- 1 172.21.141.11 192.168.0.111\" ] ] }","title":"Step 2: Verify underlay and overlay"},{"location":"task5-day2-operation/#step-3-baseline-configuration-comparison","text":"In this section we will compare the running configuration with baseline configuration for configuration compliance check. The configuration file that we backed in tak 1 will be used as baseline configuration. In this playbook, you will use \u201clookup\u201d module to find the backup filename generated in Step 1. Then you will use diff_against function in nxos_config module to compare running configuration. On Atom, Open up the project folder LTRDCN-1572 and create new file under \u201cLTRDCN-1572\u201d . Name the new file verify_config.yml and enter below data in this playbook: --- - hosts: jinja2_leaf,leaf,jinja2_spine,spine connection: local gather_facts: flase vars: nxos_provider: transport: nxapi username: \"{{ user }}\" password: \"{{ pwd }}\" timeout: 30 host: \"{{ inventory_hostname }}\" filename: \"{{ lookup('pipe', 'ls backup/{{ inventory_hostname}}_config.*')}}\" tasks: - name: configure compliance register: diff_config # when: (inventory_hostname in groups['leaf']) or (inventory_hostname in groups['jinja2_leaf']) nxos_config: provider: \"{{ nxos_provider }}\" diff_against: intended intended_config: \"{{ lookup('file', '{{filename}}') }}\" Click File and Save . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package. Before you run this playbook, SSH into leaf-4 to make some configuration changes by issuing below commands: config t no router bgp 65000 copy run start end Here is a log of execution of above command: Leaf-4# conf t Enter configuration commands, one per line. End with CNTL/Z. Leaf-4(config)# no router bgp 65000 Leaf-4(config)# copy run start [########################################] 100% Leaf-4(config)# end Leaf-4# On the Ansible server (via MTputty SSH session), run the playbook for configuration compliance check by executing ansible-playbook --diff verify_config.yml as shown below below: [root@rhel7-tools LTRDCN-1572]# ansible-playbook --diff verify_config.yml The delta between current running config and base line config are highlighted in RED from the result Below partial screenshot shows the output of above command: Bring leaf-4 back to the baseline config by executing ansible-playbook jinja2_fabric.yml --limit=198.18.4.104 command as shown below: [root@rhel7-tools LTRDCN-1572]# ansible-playbook jinja2_fabric.yml --limit=198.18.4.104 Below screenshot shows the output of above command. You can also log into leaf-4 and verify that bgp configurations are back:","title":"Step 3: Baseline configuration comparison"},{"location":"task5-day2-operation/#step-4-add-new-vni","text":"In this section, we will introduce following new VNI into the VXLAN fabric. VLAN ID VLAN Name VNI IP_Add mask Mcast 200 L2-VNI-200-Tenant1 50200 172.21.200.1 24 239.0.0.200 201 L2-VNI-201-Tenant1 50201 172.21.201.1 24 239.0.0.201 First we will creat a new role, and name it \u201cvni_provision\u201d under folder roles using ansible-galaxy using below commands on the Ansible node (using MTputty via SSH connection): [root@rhel7-tools LTRDCN-1572]# cd roles/ [root@rhel7-tools roles]# ansible-galaxy init vni_provision Verify vni_provision was created successfully Ansible-galaxy init will create new role with base role structure and empty main.yml file as role requires. Switch to \u201cAtom\u201d and sync the new created folders between Ansible node and remote desktop. Right click on project folder \u201cLTRDCN-1572\u201d , open \u201cRemote Sync\u201d select \u201cDownload Folder\u201d Edit variable file main.yml for \u201cvni_provision\u201d role under \u201c /root/LTRDCN-1572/roles/vni_provision/vars \u201d and enter below data. Make sure to click File and Save on Atom to push this to Ansible server: --- # vars file for vni_provision nxos_provider: username: \"{{ user }}\" password: \"{{ pwd }}\" transport: nxapi timeout: 30 host: \"{{ inventory_hostname }}\" L2VNI: - { vlan_id: 200, vni: 50200, ip_add: 172.21.200.1, mask: 24, vlan_name: L2-VNI-200-Tenant1, mcast: 239.0.0.200 } - { vlan_id: 201, vni: 50201, ip_add: 172.21.201.1, mask: 24, vlan_name: L2-VNI-201-Tenant1, mcast: 239.0.0.201 } Edit playbook file main.yml for \u201cvni_provision\u201d role under \u201c/root/LTRDCN-1572/roles/vni_provision/tasks\u201d and enter below data. Make sure to click File and Save on Atom to push this to Ansible server: --- # tasks file for vni_provision - name: Configure VLAN to VNI nxos_vlan: vlan_id: \"{{ item.vlan_id }}\" mapped_vni: \"{{ item.vni }}\" name: \"{{ item.vlan_name }}\" provider: \"{{ nxos_provider }}\" with_items: - \"{{ L2VNI }}\" - name: Configure L2VNI nxos_interface: interface: vlan\"{{ item.vlan_id }}\" provider: \"{{ nxos_provider }}\" with_items: \"{{ L2VNI }}\" - name: Assign interface to Tenant VRF nxos_vrf_interface: vrf: Tenant-1 interface: \"vlan{{ item.vlan_id }}\" provider: \"{{ nxos_provider }}\" with_items: - \"{{ L2VNI }}\" - name: Configure SVI IP nxos_ip_interface: interface: \"vlan{{ item.vlan_id }}\" addr: \"{{ item.ip_add }}\" mask: \"{{ item.mask }}\" provider: \"{{ nxos_provider }}\" with_items: \"{{ L2VNI }}\" - name: Configure L2VNI SVI nxos_interface: interface: vlan\"{{ item.vlan_id }}\" fabric_forwarding_anycast_gateway: true provider: \"{{ nxos_provider }}\" with_items: \"{{ L2VNI }}\" - name: Configure L2VNI to VTEP nxos_vxlan_vtep_vni: interface: nve1 vni: \"{{ item.vni }}\" multicast_group: \"{{ item.mcast }}\" provider: \"{{ nxos_provider }}\" with_items: \"{{ L2VNI }}\" - name: Configure L2VNI RD/RT nxos_evpn_vni: vni: \"{{ item.vni }}\" route_distinguisher: auto route_target_both: auto provider: \"{{ nxos_provider }}\" with_items: \"{{ L2VNI }}\" this is shown in below screenshot: Switch to \u201cAtom\u201d create new playbook \u2018vni_provision.yml\u2019 under project folder LTRDCN-1572 and enter below data. Make sure to click File and Save on Atom to push this to Ansible server: --- - hosts: leaf,jinja2_leaf connection: local roles: - vni_provision Run playbook vni_provision.yml to add new VNIs on the fabric by issuing ansible-playbook vni_provision.yml command as shown below: [root@rhel7-tools LTRDCN-1572]# ansible-playbook vni_provision.yml Below screenshot shows the output of above command: Switch to MTPutty and connect to leaf-4 (SSH connection0, verify the change on leaf switches by issuing below command: show nve vni Notice the new created L2VNI as shown in below screenshot:","title":"Step 4: Add new VNI"},{"location":"task5-day2-operation/#congratulation-you-have-completed-vxlan-fabric-lab","text":"","title":"Congratulation! You have completed VXLAN Fabric Lab."},{"location":"pic/Readme/","text":"pic files","title":"Readme"}]}